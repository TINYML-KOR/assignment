{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2tFjnZZVlIFL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8a7dae16d9014c249dc95527633a2998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_078a83e02af24ef681c98c5188b1c020",
              "IPY_MODEL_d59dd347a94e46208403a97066d2832e",
              "IPY_MODEL_dd0bd33c51134465b22ce04cd161a27d"
            ],
            "layout": "IPY_MODEL_ce8e399051284c6196775a92c1fc8b21"
          }
        },
        "078a83e02af24ef681c98c5188b1c020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73edc238b6424433872465dc930a0df6",
            "placeholder": "​",
            "style": "IPY_MODEL_139d20410ffa434e91f08064e68ec17f",
            "value": "eval:  95%"
          }
        },
        "d59dd347a94e46208403a97066d2832e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9e2eedd9d24426ba6a5ce1dba83f018",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d3b4775e6ffb464aac100ce2dcb82c3d",
            "value": 20
          }
        },
        "dd0bd33c51134465b22ce04cd161a27d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03053cca4c4b402b8a36618176fd0628",
            "placeholder": "​",
            "style": "IPY_MODEL_a9b218674e1b4ac3b37da060d4e611a0",
            "value": " 19/20 [00:01&lt;00:00, 10.90it/s]"
          }
        },
        "ce8e399051284c6196775a92c1fc8b21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "73edc238b6424433872465dc930a0df6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "139d20410ffa434e91f08064e68ec17f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9e2eedd9d24426ba6a5ce1dba83f018": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3b4775e6ffb464aac100ce2dcb82c3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03053cca4c4b402b8a36618176fd0628": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b218674e1b4ac3b37da060d4e611a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df8fb43d752645a1bdb334933197c31b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b47d0da90e244359fdf53629485f8fd",
              "IPY_MODEL_20bbbcbc5a8d4045849b13764281b462",
              "IPY_MODEL_15f415670c72409f88342cfe98c5d116"
            ],
            "layout": "IPY_MODEL_3d22bbfc716c4a39a9b51bc785987de9"
          }
        },
        "6b47d0da90e244359fdf53629485f8fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27ffbdafd1754e318f65a84975ee1f67",
            "placeholder": "​",
            "style": "IPY_MODEL_a1ee49dab3184480834ec8a6c5b71745",
            "value": "eval:  95%"
          }
        },
        "20bbbcbc5a8d4045849b13764281b462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e25b4ed545b44b2a2ce03f1f2261230",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f78c659010df4f45b64592380cf0ef22",
            "value": 20
          }
        },
        "15f415670c72409f88342cfe98c5d116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab8db668bb4240b8957ad20be7f52c2a",
            "placeholder": "​",
            "style": "IPY_MODEL_60afd4f777ba444988da504f0fe409c7",
            "value": " 19/20 [00:01&lt;00:00, 12.95it/s]"
          }
        },
        "3d22bbfc716c4a39a9b51bc785987de9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "27ffbdafd1754e318f65a84975ee1f67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1ee49dab3184480834ec8a6c5b71745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e25b4ed545b44b2a2ce03f1f2261230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f78c659010df4f45b64592380cf0ef22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab8db668bb4240b8957ad20be7f52c2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60afd4f777ba444988da504f0fe409c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e9f43aeb2d64d599f237baece19a247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04f16e4c89cc4b3b867ec12749abf44e",
              "IPY_MODEL_d822ecda1d694ad78ed28c7d687b58a1",
              "IPY_MODEL_9ac77c3f1b584160aa5cb6fe609ad948"
            ],
            "layout": "IPY_MODEL_822630d35cda48659af632c457970fcf"
          }
        },
        "04f16e4c89cc4b3b867ec12749abf44e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aceb8964b9b649fa8a1e1edff968a1aa",
            "placeholder": "​",
            "style": "IPY_MODEL_a3e69fe342974aa4a4795a949c59fa61",
            "value": "eval:  95%"
          }
        },
        "d822ecda1d694ad78ed28c7d687b58a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfa39903549d432f969a4d6935c15cb1",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2ec8b4eef1c4f288b9cfadadb090e1d",
            "value": 20
          }
        },
        "9ac77c3f1b584160aa5cb6fe609ad948": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b131260245e4fed9d390ec74a542d19",
            "placeholder": "​",
            "style": "IPY_MODEL_75353c9908bb4e2a93de2648990d0628",
            "value": " 19/20 [00:01&lt;00:00, 12.94it/s]"
          }
        },
        "822630d35cda48659af632c457970fcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "aceb8964b9b649fa8a1e1edff968a1aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3e69fe342974aa4a4795a949c59fa61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfa39903549d432f969a4d6935c15cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2ec8b4eef1c4f288b9cfadadb090e1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b131260245e4fed9d390ec74a542d19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75353c9908bb4e2a93de2648990d0628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75740d5554bc46f9951d177a66f28af8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c325fd3fd6446c09226d8d12b337904",
              "IPY_MODEL_0be881a2000b4af98253f273ff138e9b",
              "IPY_MODEL_4b01f22136404172be0ce2057f436823"
            ],
            "layout": "IPY_MODEL_559b2ce0b1dd4fa5b7ed4e26db2178f1"
          }
        },
        "3c325fd3fd6446c09226d8d12b337904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b77dc98c8674896a5edc30aa8b649d1",
            "placeholder": "​",
            "style": "IPY_MODEL_5bcc87a3557340e9b3b1c3d3ad2741b6",
            "value": "eval:  95%"
          }
        },
        "0be881a2000b4af98253f273ff138e9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_907cd9102355486c9e12f5d5cd8019eb",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d21baa3a17841b8ae2498179f9366d8",
            "value": 20
          }
        },
        "4b01f22136404172be0ce2057f436823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad42172bf8324cf2aabe99bbfab75fdf",
            "placeholder": "​",
            "style": "IPY_MODEL_156a6c48869e48d58263a0c9856a70c9",
            "value": " 19/20 [00:01&lt;00:00, 10.83it/s]"
          }
        },
        "559b2ce0b1dd4fa5b7ed4e26db2178f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "9b77dc98c8674896a5edc30aa8b649d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bcc87a3557340e9b3b1c3d3ad2741b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "907cd9102355486c9e12f5d5cd8019eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d21baa3a17841b8ae2498179f9366d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad42172bf8324cf2aabe99bbfab75fdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "156a6c48869e48d58263a0c9856a70c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Colab Badge](http://img.shields.io/badge/Colaboratory-black?style=for-the-badge&logo=google-colab)](https://colab.research.google.com/github/TINYML-KOR/assignment/blob/main/Lab2_cureiuxjy.ipynb)"
      ],
      "metadata": {
        "id": "EIci33XDTa5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MIT 6.5940 EfficientML.ai Fall 2023 Lab 2: Quantization**"
      ],
      "metadata": {
        "id": "pmi4V4VaYR6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab notebook provides code and a framework for Lab 2 quantization. You can work out your solutions here."
      ],
      "metadata": {
        "id": "_PC9ifUhHwRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please fill out this [feedback form](https://forms.gle/ZeCH5anNPrkd5wpp7) when you finished this lab. We would love to hear your thoughts or feedback on how we can improve this lab!"
      ],
      "metadata": {
        "id": "A-antxp8SSyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goals\n",
        "\n",
        "이 과제에서는 모델 크기와 지연 시간을 줄이기 위해 클래식한 **neural network model**을 **quantizing**하는 연습을 할 것입니다. 이 과제의 목표는 다음과 같습니다:\n",
        "\n",
        "- **Quantization**의 기본 개념을 이해합니다.\n",
        "- **k-means quantization**을 구현하고 적용합니다.\n",
        "- k-means **quantization**에 대해 **quantization-aware training**을 구현하고 적용합니다.\n",
        "- **linear quantization**을 구현하고 적용합니다.\n",
        "- linear **quantization**에 대해 **integer-only inference**를 구현하고 적용합니다.\n",
        "- **Quantization**에서의 성능 개선(예: 속도 향상)에 대한 기본적인 이해를 얻습니다.\n",
        "- 이러한 **quantization** 접근 방식 사이의 차이점과 트레이드오프를 이해합니다.\n"
      ],
      "metadata": {
        "id": "vhVOMmbAaHRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contents\n",
        "\n",
        "주요 섹션은 2가지: ***K-Means Quantization*** 과 ***Linear Quantization***\n",
        "\n",
        "총 ***10***개의 질문이 있습니다:\n",
        "- *K-Means Quantization*에 대해서는 ***3***개의 질문이 있습니다 (질문 1-3).\n",
        "- *Linear Quantization*에 대해서는 ***6***개의 질문이 있습니다 (질문 4-9).\n",
        "- 질문 10은 k-means quantization과 linear quantization을 비교합니다."
      ],
      "metadata": {
        "id": "W6HPdGZ7aHZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "2tFjnZZVlIFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, install the required packages and download the datasets and pretrained model. Here we use CIFAR10 dataset and VGG network which is the same as what we used in the Lab 0 tutorial.\n",
        "\n"
      ],
      "metadata": {
        "id": "Bz16rxaSH_7f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nyngBRTXQG2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02806952-cfe0-48f9-d1bf-2d84de44c9da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing torchprofile...\n",
            "Installing fast-pytorch-kmeans...\n",
            "All required packages have been successfully installed!\n"
          ]
        }
      ],
      "source": [
        "print('Installing torchprofile...')\n",
        "!pip install torchprofile 1>/dev/null\n",
        "print('Installing fast-pytorch-kmeans...')\n",
        "! pip install fast-pytorch-kmeans 1>/dev/null\n",
        "print('All required packages have been successfully installed!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import math\n",
        "import random\n",
        "from collections import OrderedDict, defaultdict\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import *\n",
        "from torch.optim.lr_scheduler import *\n",
        "from torch.utils.data import DataLoader\n",
        "from torchprofile import profile_macs\n",
        "from torchvision.datasets import *\n",
        "from torchvision.transforms import *\n",
        "\n",
        "from torchprofile import profile_macs\n",
        "\n",
        "assert torch.cuda.is_available(), \\\n",
        "\"The current runtime does not have CUDA support.\" \\\n",
        "\"Please go to menu bar (Runtime - Change runtime type) and select GPU\""
      ],
      "metadata": {
        "id": "zDWoVhv_wGmA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "nLcJUofTDKud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4157c924-3698-4de0-d19f-2ef252c1c882"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7829ac32c610>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_url(url, model_dir='.', overwrite=False):\n",
        "    import os, sys\n",
        "    from urllib.request import urlretrieve\n",
        "    target_dir = url.split('/')[-1]\n",
        "    model_dir = os.path.expanduser(model_dir)\n",
        "    try:\n",
        "        if not os.path.exists(model_dir):\n",
        "            os.makedirs(model_dir)\n",
        "        model_dir = os.path.join(model_dir, target_dir)\n",
        "        cached_file = model_dir\n",
        "        if not os.path.exists(cached_file) or overwrite:\n",
        "            sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\n",
        "            urlretrieve(url, cached_file)\n",
        "        return cached_file\n",
        "    except Exception as e:\n",
        "        # remove lock file so download can be executed next time.\n",
        "        os.remove(os.path.join(model_dir, 'download.lock'))\n",
        "        sys.stderr.write('Failed to download from url %s' % url + '\\n' + str(e) + '\\n')\n",
        "        return None"
      ],
      "metadata": {
        "id": "6kpu78GyHGA-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    layers = []\n",
        "    counts = defaultdict(int)\n",
        "\n",
        "    def add(name: str, layer: nn.Module) -> None:\n",
        "      layers.append((f\"{name}{counts[name]}\", layer))\n",
        "      counts[name] += 1\n",
        "\n",
        "    in_channels = 3\n",
        "    for x in self.ARCH:\n",
        "      if x != 'M':\n",
        "        # conv-bn-relu\n",
        "        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
        "        add(\"bn\", nn.BatchNorm2d(x))\n",
        "        add(\"relu\", nn.ReLU(True))\n",
        "        in_channels = x\n",
        "      else:\n",
        "        # maxpool\n",
        "        add(\"pool\", nn.MaxPool2d(2))\n",
        "    add(\"avgpool\", nn.AvgPool2d(2))\n",
        "    self.backbone = nn.Sequential(OrderedDict(layers))\n",
        "    self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
        "    x = self.backbone(x)\n",
        "\n",
        "    # avgpool: [N, 512, 2, 2] => [N, 512]\n",
        "    # x = x.mean([2, 3])\n",
        "    x = x.view(x.shape[0], -1)\n",
        "\n",
        "    # classifier: [N, 512] => [N, 10]\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "qqInscyoifYN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "  model: nn.Module,\n",
        "  dataloader: DataLoader,\n",
        "  criterion: nn.Module,\n",
        "  optimizer: Optimizer,\n",
        "  scheduler: LambdaLR,\n",
        "  callbacks = None\n",
        ") -> None:\n",
        "  model.train()\n",
        "\n",
        "  for inputs, targets in tqdm(dataloader, desc='train', leave=False):\n",
        "    # Move the data from CPU to GPU\n",
        "    inputs = inputs.cuda()\n",
        "    targets = targets.cuda()\n",
        "\n",
        "    # Reset the gradients (from the last iteration)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward inference\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    # Backward propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update optimizer and LR scheduler\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    if callbacks is not None:\n",
        "        for callback in callbacks:\n",
        "            callback()"
      ],
      "metadata": {
        "id": "WqnPt0LUEaWi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def evaluate(\n",
        "  model: nn.Module,\n",
        "  dataloader: DataLoader,\n",
        "  extra_preprocess = None\n",
        ") -> float:\n",
        "  model.eval()\n",
        "\n",
        "  num_samples = 0\n",
        "  num_correct = 0\n",
        "\n",
        "  for inputs, targets in tqdm(dataloader, desc=\"eval\", leave=False):\n",
        "    # Move the data from CPU to GPU\n",
        "    inputs = inputs.cuda()\n",
        "    if extra_preprocess is not None:\n",
        "        for preprocess in extra_preprocess:\n",
        "            inputs = preprocess(inputs)\n",
        "\n",
        "    targets = targets.cuda()\n",
        "\n",
        "    # Inference\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Convert logits to class indices\n",
        "    outputs = outputs.argmax(dim=1)\n",
        "\n",
        "    # Update metrics\n",
        "    num_samples += targets.size(0)\n",
        "    num_correct += (outputs == targets).sum()\n",
        "\n",
        "  return (num_correct / num_samples * 100).item()"
      ],
      "metadata": {
        "id": "wVA1_oeUEUf6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helpler Functions (Flops, Model Size calculation, etc.)"
      ],
      "metadata": {
        "id": "QBBKNhKNlAwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_flops(model, inputs):\n",
        "    num_macs = profile_macs(model, inputs)\n",
        "    return num_macs"
      ],
      "metadata": {
        "id": "mRdK_ThzlMxL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size(model: nn.Module, data_width=32):\n",
        "    \"\"\"\n",
        "    calculate the model size in bits\n",
        "    :param data_width: #bits per element\n",
        "    \"\"\"\n",
        "    num_elements = 0\n",
        "    for param in model.parameters():\n",
        "        num_elements += param.numel()\n",
        "    return num_elements * data_width\n",
        "\n",
        "Byte = 8\n",
        "KiB = 1024 * Byte\n",
        "MiB = 1024 * KiB\n",
        "GiB = 1024 * MiB"
      ],
      "metadata": {
        "id": "cepv4SUalU79"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define misc funcions for verification."
      ],
      "metadata": {
        "id": "CGhomDjsaDB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_k_means_quantize(\n",
        "    test_tensor=torch.tensor([\n",
        "        [-0.3747,  0.0874,  0.3200, -0.4868,  0.4404],\n",
        "        [-0.0402,  0.2322, -0.2024, -0.4986,  0.1814],\n",
        "        [ 0.3102, -0.3942, -0.2030,  0.0883, -0.4741],\n",
        "        [-0.1592, -0.0777, -0.3946, -0.2128,  0.2675],\n",
        "        [ 0.0611, -0.1933, -0.4350,  0.2928, -0.1087]]),\n",
        "    bitwidth=2):\n",
        "    def plot_matrix(tensor, ax, title, cmap=ListedColormap(['white'])):\n",
        "        ax.imshow(tensor.cpu().numpy(), vmin=-0.5, vmax=0.5, cmap=cmap)\n",
        "        ax.set_title(title)\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_xticklabels([])\n",
        "        for i in range(tensor.shape[1]):\n",
        "            for j in range(tensor.shape[0]):\n",
        "                text = ax.text(j, i, f'{tensor[i, j].item():.2f}',\n",
        "                                ha=\"center\", va=\"center\", color=\"k\")\n",
        "\n",
        "    fig, axes = plt.subplots(1,2, figsize=(8, 12))\n",
        "    ax_left, ax_right = axes.ravel()\n",
        "\n",
        "    print(test_tensor)\n",
        "    plot_matrix(test_tensor, ax_left, 'original tensor')\n",
        "\n",
        "    num_unique_values_before_quantization = test_tensor.unique().numel()\n",
        "    k_means_quantize(test_tensor, bitwidth=bitwidth)\n",
        "    num_unique_values_after_quantization = test_tensor.unique().numel()\n",
        "    print('* Test k_means_quantize()')\n",
        "    print(f'    target bitwidth: {bitwidth} bits')\n",
        "    print(f'        num unique values before k-means quantization: {num_unique_values_before_quantization}')\n",
        "    print(f'        num unique values after  k-means quantization: {num_unique_values_after_quantization}')\n",
        "    assert num_unique_values_after_quantization == min((1 << bitwidth), num_unique_values_before_quantization)\n",
        "    print('* Test passed.')\n",
        "\n",
        "    plot_matrix(test_tensor, ax_right, f'{bitwidth}-bit k-means quantized tensor', cmap='tab20c')\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WOJBXVXQeCXF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_linear_quantize(\n",
        "    test_tensor=torch.tensor([\n",
        "        [ 0.0523,  0.6364, -0.0968, -0.0020,  0.1940],\n",
        "        [ 0.7500,  0.5507,  0.6188, -0.1734,  0.4677],\n",
        "        [-0.0669,  0.3836,  0.4297,  0.6267, -0.0695],\n",
        "        [ 0.1536, -0.0038,  0.6075,  0.6817,  0.0601],\n",
        "        [ 0.6446, -0.2500,  0.5376, -0.2226,  0.2333]]),\n",
        "    quantized_test_tensor=torch.tensor([\n",
        "        [-1,  1, -1, -1,  0],\n",
        "        [ 1,  1,  1, -2,  0],\n",
        "        [-1,  0,  0,  1, -1],\n",
        "        [-1, -1,  1,  1, -1],\n",
        "        [ 1, -2,  1, -2,  0]], dtype=torch.int8),\n",
        "    real_min=-0.25, real_max=0.75, bitwidth=2, scale=1/3, zero_point=-1):\n",
        "    def plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=ListedColormap(['white'])):\n",
        "        ax.imshow(tensor.cpu().numpy(), vmin=vmin, vmax=vmax, cmap=cmap)\n",
        "        ax.set_title(title)\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_xticklabels([])\n",
        "        for i in range(tensor.shape[0]):\n",
        "            for j in range(tensor.shape[1]):\n",
        "                datum = tensor[i, j].item()\n",
        "                if isinstance(datum, float):\n",
        "                    text = ax.text(j, i, f'{datum:.2f}',\n",
        "                                    ha=\"center\", va=\"center\", color=\"k\")\n",
        "                else:\n",
        "                    text = ax.text(j, i, f'{datum}',\n",
        "                                    ha=\"center\", va=\"center\", color=\"k\")\n",
        "    quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
        "    fig, axes = plt.subplots(1,3, figsize=(10, 32))\n",
        "    plot_matrix(test_tensor, axes[0], 'original tensor', vmin=real_min, vmax=real_max)\n",
        "    _quantized_test_tensor = linear_quantize(\n",
        "        test_tensor, bitwidth=bitwidth, scale=scale, zero_point=zero_point)\n",
        "    _reconstructed_test_tensor = scale * (_quantized_test_tensor.float() - zero_point)\n",
        "    print('* Test linear_quantize()')\n",
        "    print(f'    target bitwidth: {bitwidth} bits')\n",
        "    print(f'        scale: {scale}')\n",
        "    print(f'        zero point: {zero_point}')\n",
        "    assert _quantized_test_tensor.equal(quantized_test_tensor)\n",
        "    print('* Test passed.')\n",
        "    plot_matrix(_quantized_test_tensor, axes[1], f'2-bit linear quantized tensor',\n",
        "                vmin=quantized_min, vmax=quantized_max, cmap='tab20c')\n",
        "    plot_matrix(_reconstructed_test_tensor, axes[2], f'reconstructed tensor',\n",
        "                vmin=real_min, vmax=real_max, cmap='tab20c')\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "jI92NMafaCk7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_quantized_fc(\n",
        "    input=torch.tensor([\n",
        "        [0.6118, 0.7288, 0.8511, 0.2849, 0.8427, 0.7435, 0.4014, 0.2794],\n",
        "        [0.3676, 0.2426, 0.1612, 0.7684, 0.6038, 0.0400, 0.2240, 0.4237],\n",
        "        [0.6565, 0.6878, 0.4670, 0.3470, 0.2281, 0.8074, 0.0178, 0.3999],\n",
        "        [0.1863, 0.3567, 0.6104, 0.0497, 0.0577, 0.2990, 0.6687, 0.8626]]),\n",
        "    weight=torch.tensor([\n",
        "        [ 1.2626e-01, -1.4752e-01,  8.1910e-02,  2.4982e-01, -1.0495e-01,\n",
        "         -1.9227e-01, -1.8550e-01, -1.5700e-01],\n",
        "        [ 2.7624e-01, -4.3835e-01,  5.1010e-02, -1.2020e-01, -2.0344e-01,\n",
        "          1.0202e-01, -2.0799e-01,  2.4112e-01],\n",
        "        [-3.8216e-01, -2.8047e-01,  8.5238e-02, -4.2504e-01, -2.0952e-01,\n",
        "          3.2018e-01, -3.3619e-01,  2.0219e-01],\n",
        "        [ 8.9233e-02, -1.0124e-01,  1.1467e-01,  2.0091e-01,  1.1438e-01,\n",
        "         -4.2427e-01,  1.0178e-01, -3.0941e-04],\n",
        "        [-1.8837e-02, -2.1256e-01, -4.5285e-01,  2.0949e-01, -3.8684e-01,\n",
        "         -1.7100e-01, -4.5331e-01, -2.0433e-01],\n",
        "        [-2.0038e-01, -5.3757e-02,  1.8997e-01, -3.6866e-01,  5.5484e-02,\n",
        "          1.5643e-01, -2.3538e-01,  2.1103e-01],\n",
        "        [-2.6875e-01,  2.4984e-01, -2.3514e-01,  2.5527e-01,  2.0322e-01,\n",
        "          3.7675e-01,  6.1563e-02,  1.7201e-01],\n",
        "        [ 3.3541e-01, -3.3555e-01, -4.3349e-01,  4.3043e-01, -2.0498e-01,\n",
        "         -1.8366e-01, -9.1553e-02, -4.1168e-01]]),\n",
        "    bias=torch.tensor([ 0.1954, -0.2756,  0.3113,  0.1149,  0.4274,  0.2429, -0.1721, -0.2502]),\n",
        "    quantized_bias=torch.tensor([ 3, -2,  3,  1,  3,  2, -2, -2], dtype=torch.int32),\n",
        "    shifted_quantized_bias=torch.tensor([-1,  0, -3, -1, -3,  0,  2, -4], dtype=torch.int32),\n",
        "    calc_quantized_output=torch.tensor([\n",
        "        [ 0, -1,  0, -1, -1,  0,  1, -2],\n",
        "        [ 0,  0, -1,  0,  0,  0,  0, -1],\n",
        "        [ 0,  0,  0, -1,  0,  0,  0, -1],\n",
        "        [ 0,  0,  0,  0,  0,  1, -1, -2]], dtype=torch.int8),\n",
        "    bitwidth=2, batch_size=4, in_channels=8, out_channels=8):\n",
        "    def plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=ListedColormap(['white'])):\n",
        "        ax.imshow(tensor.cpu().numpy(), vmin=vmin, vmax=vmax, cmap=cmap)\n",
        "        ax.set_title(title)\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_xticklabels([])\n",
        "        for i in range(tensor.shape[0]):\n",
        "            for j in range(tensor.shape[1]):\n",
        "                datum = tensor[i, j].item()\n",
        "                if isinstance(datum, float):\n",
        "                    text = ax.text(j, i, f'{datum:.2f}',\n",
        "                                    ha=\"center\", va=\"center\", color=\"k\")\n",
        "                else:\n",
        "                    text = ax.text(j, i, f'{datum}',\n",
        "                                    ha=\"center\", va=\"center\", color=\"k\")\n",
        "\n",
        "    output = torch.nn.functional.linear(input, weight, bias)\n",
        "\n",
        "    quantized_weight, weight_scale, weight_zero_point = \\\n",
        "        linear_quantize_weight_per_channel(weight, bitwidth)\n",
        "    quantized_input, input_scale, input_zero_point = \\\n",
        "        linear_quantize_feature(input, bitwidth)\n",
        "    _quantized_bias, bias_scale, bias_zero_point = \\\n",
        "        linear_quantize_bias_per_output_channel(bias, weight_scale, input_scale)\n",
        "    assert _quantized_bias.equal(_quantized_bias)\n",
        "    _shifted_quantized_bias = \\\n",
        "        shift_quantized_linear_bias(quantized_bias, quantized_weight, input_zero_point)\n",
        "    assert _shifted_quantized_bias.equal(shifted_quantized_bias)\n",
        "    quantized_output, output_scale, output_zero_point = \\\n",
        "        linear_quantize_feature(output, bitwidth)\n",
        "\n",
        "    _calc_quantized_output = quantized_linear(\n",
        "        quantized_input, quantized_weight, shifted_quantized_bias,\n",
        "        bitwidth, bitwidth,\n",
        "        input_zero_point, output_zero_point,\n",
        "        input_scale, weight_scale, output_scale)\n",
        "    assert _calc_quantized_output.equal(calc_quantized_output)\n",
        "\n",
        "    reconstructed_weight = weight_scale * (quantized_weight.float() - weight_zero_point)\n",
        "    reconstructed_input = input_scale * (quantized_input.float() - input_zero_point)\n",
        "    reconstructed_bias = bias_scale * (quantized_bias.float() - bias_zero_point)\n",
        "    reconstructed_calc_output = output_scale * (calc_quantized_output.float() - output_zero_point)\n",
        "\n",
        "    fig, axes = plt.subplots(3,3, figsize=(15, 12))\n",
        "    quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
        "    plot_matrix(weight, axes[0, 0], 'original weight', vmin=-0.5, vmax=0.5)\n",
        "    plot_matrix(input.t(), axes[1, 0], 'original input', vmin=0, vmax=1)\n",
        "    plot_matrix(output.t(), axes[2, 0], 'original output', vmin=-1.5, vmax=1.5)\n",
        "    plot_matrix(quantized_weight, axes[0, 1], f'{bitwidth}-bit linear quantized weight',\n",
        "                vmin=quantized_min, vmax=quantized_max, cmap='tab20c')\n",
        "    plot_matrix(quantized_input.t(), axes[1, 1], f'{bitwidth}-bit linear quantized input',\n",
        "                vmin=quantized_min, vmax=quantized_max, cmap='tab20c')\n",
        "    plot_matrix(calc_quantized_output.t(), axes[2, 1], f'quantized output from quantized_linear()',\n",
        "                vmin=quantized_min, vmax=quantized_max, cmap='tab20c')\n",
        "    plot_matrix(reconstructed_weight, axes[0, 2], f'reconstructed weight',\n",
        "                vmin=-0.5, vmax=0.5, cmap='tab20c')\n",
        "    plot_matrix(reconstructed_input.t(), axes[1, 2], f'reconstructed input',\n",
        "                vmin=0, vmax=1, cmap='tab20c')\n",
        "    plot_matrix(reconstructed_calc_output.t(), axes[2, 2], f'reconstructed output',\n",
        "                vmin=-1.5, vmax=1.5, cmap='tab20c')\n",
        "\n",
        "    print('* Test quantized_fc()')\n",
        "    print(f'    target bitwidth: {bitwidth} bits')\n",
        "    print(f'      batch size: {batch_size}')\n",
        "    print(f'      input channels: {in_channels}')\n",
        "    print(f'      output channels: {out_channels}')\n",
        "    print('* Test passed.')\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vK-56lHteduz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Pretrained Model"
      ],
      "metadata": {
        "id": "mDDp0OWLh6vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_url = \"https://hanlab18.mit.edu/files/course/labs/vgg.cifar.pretrained.pth\"\n",
        "checkpoint = torch.load(download_url(checkpoint_url), map_location=\"cpu\")\n",
        "model = VGG().cuda()\n",
        "print(f\"=> loading checkpoint '{checkpoint_url}'\")\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "recover_model = lambda : model.load_state_dict(checkpoint['state_dict'])"
      ],
      "metadata": {
        "id": "oNEYAZ_TQf7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3baa06c-fe1d-42a9-e0a2-7bffbd0ac52a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> loading checkpoint 'https://hanlab18.mit.edu/files/course/labs/vgg.cifar.pretrained.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 32\n",
        "transforms = {\n",
        "    \"train\": Compose([\n",
        "        RandomCrop(image_size, padding=4),\n",
        "        RandomHorizontalFlip(),\n",
        "        ToTensor(),\n",
        "    ]),\n",
        "    \"test\": ToTensor(),\n",
        "}\n",
        "dataset = {}\n",
        "for split in [\"train\", \"test\"]:\n",
        "  dataset[split] = CIFAR10(\n",
        "    root=\"data/cifar10\",\n",
        "    train=(split == \"train\"),\n",
        "    download=True,\n",
        "    transform=transforms[split],\n",
        "  )\n",
        "dataloader = {}\n",
        "for split in ['train', 'test']:\n",
        "  dataloader[split] = DataLoader(\n",
        "    dataset[split],\n",
        "    batch_size=512,\n",
        "    shuffle=(split == 'train'),\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "  )"
      ],
      "metadata": {
        "id": "jQb3_6zlQfib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa1104aa-9c2c-4226-b42c-d9688f02497a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 먼저 FP32 Model의 정확도와 모델 크기를 평가해봅시다\n"
      ],
      "metadata": {
        "id": "rjESsg5nkdBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fp32_model_accuracy = evaluate(model, dataloader['test'])\n",
        "fp32_model_size = get_model_size(model)\n",
        "print(f\"fp32 model has accuracy={fp32_model_accuracy:.2f}%\")\n",
        "print(f\"fp32 model has size={fp32_model_size/MiB:.2f} MiB\")"
      ],
      "metadata": {
        "id": "DTiA8hxMkkbU",
        "outputId": "25c89a22-4c44-46f0-8b45-f09d9c774f72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52,
          "referenced_widgets": [
            "8a7dae16d9014c249dc95527633a2998",
            "078a83e02af24ef681c98c5188b1c020",
            "d59dd347a94e46208403a97066d2832e",
            "dd0bd33c51134465b22ce04cd161a27d",
            "ce8e399051284c6196775a92c1fc8b21",
            "73edc238b6424433872465dc930a0df6",
            "139d20410ffa434e91f08064e68ec17f",
            "f9e2eedd9d24426ba6a5ce1dba83f018",
            "d3b4775e6ffb464aac100ce2dcb82c3d",
            "03053cca4c4b402b8a36618176fd0628",
            "a9b218674e1b4ac3b37da060d4e611a0"
          ]
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "eval:   0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a7dae16d9014c249dc95527633a2998"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fp32 model has accuracy=92.95%\n",
            "fp32 model has size=35.20 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means Quantization"
      ],
      "metadata": {
        "id": "6QdGiddu87p2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network quantization**은 deep network를 표현하는 데 필요한 가중치 당 비트(bits per weight) 수를 줄여 네트워크를 압축합니다. **quantized network**는 하드웨어 지원이 있을 경우 더 빠른 추론 속도를 가질 수 있습니다.\n"
      ],
      "metadata": {
        "id": "Gr0MvPoVTirU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 섹션에서는 [Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization And Huffman Coding](https://arxiv.org/pdf/1510.00149.pdf)에서처럼 신경망에 대한 **K-means quantization**을 탐구할 것입니다.\n"
      ],
      "metadata": {
        "id": "lzlKpiNvY3lc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![kmeans.png](/images/lab2/kmeans.png)"
      ],
      "metadata": {
        "id": "KDoR8iXP0VDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ***quantized_weight* = *codebook.centroids*\\[*codebook.labels*\\].view_as(weight)**\n",
        "\n",
        "$n$-bit k-means **quantization**은 시냅스를 $2^n$ 개의 클러스터로 나누고, 동일한 클러스터 내의 시냅스는 동일한 가중치 값을 공유하게 됩니다.\n",
        "\n",
        "따라서, k-means **quantization**은 다음과 같은 codebook을 생성합니다:\n",
        "* `centroids`: $2^n$ fp32 클러스터 중심.\n",
        "* `labels`: 원래 fp32 가중치 텐서와 동일한 #elements를 가진 $n$-bit 정수 텐서. 각 정수는 해당 클러스터가 어디에 속하는지를 나타냅니다.\n",
        "\n",
        "추론하는 동안, codebook을 기반으로 한 fp32 텐서가 추론을 위해 생성됩니다:\n",
        "\n",
        "> ***quantized_weight* = *codebook.centroids*\\[*codebook.labels*\\].view_as(weight)**\n"
      ],
      "metadata": {
        "id": "mGkEk4CX0VIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "Codebook = namedtuple('Codebook', ['centroids', 'labels'])"
      ],
      "metadata": {
        "id": "Z2x1DbpfT3SD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 pts)\n",
        "\n",
        "아래의 K-Means quantization function을 완성하세요."
      ],
      "metadata": {
        "id": "f3BczeDEN0y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fast_pytorch_kmeans import KMeans\n",
        "\n",
        "def k_means_quantize(fp32_tensor: torch.Tensor, bitwidth=4, codebook=None):\n",
        "    \"\"\"\n",
        "    quantize tensor using k-means clustering\n",
        "    :param fp32_tensor:\n",
        "    :param bitwidth: [int] quantization bit width, default=4\n",
        "    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)\n",
        "    :return:\n",
        "        [Codebook = (centroids, labels)]\n",
        "            centroids: [torch.(cuda.)FloatTensor] the cluster centroids\n",
        "            labels: [torch.(cuda.)LongTensor] cluster label tensor\n",
        "    \"\"\"\n",
        "    if codebook is None:\n",
        "        ############### YOUR CODE STARTS HERE ###############\n",
        "        # get number of clusters based on the quantization precision\n",
        "        n_clusters = 2 ** bitwidth  # Calculate number of clusters as 2^bitwidth\n",
        "        ############### YOUR CODE ENDS HERE #################\n",
        "        # use k-means to get the quantization centroids\n",
        "        kmeans = KMeans(n_clusters=n_clusters, mode='euclidean', verbose=0)\n",
        "        labels = kmeans.fit_predict(fp32_tensor.view(-1, 1)).to(torch.long)\n",
        "        centroids = kmeans.centroids.to(torch.float).view(-1)\n",
        "        codebook = Codebook(centroids, labels)\n",
        "\n",
        "    ############### YOUR CODE STARTS HERE ###############\n",
        "    # decode the codebook into k-means quantized tensor for inference\n",
        "    # hint: one line of code\n",
        "    quantized_tensor = codebook.centroids[codebook.labels].view_as(fp32_tensor)\n",
        "    ############### YOUR CODE ENDS HERE #################\n",
        "    fp32_tensor.set_(quantized_tensor.view_as(fp32_tensor))\n",
        "    return codebook"
      ],
      "metadata": {
        "id": "bnZ2b4hgOnoC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위에서 작성한 k-means quantization function을 더미 텐서에 적용하여 확인해봅시다."
      ],
      "metadata": {
        "id": "XlVfIFRYPbfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_k_means_quantize()"
      ],
      "metadata": {
        "id": "WZd8z1vGPdG8",
        "outputId": "486fc925-a98a-4bc7-dc0f-0cd95bc9380b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.3747,  0.0874,  0.3200, -0.4868,  0.4404],\n",
            "        [-0.0402,  0.2322, -0.2024, -0.4986,  0.1814],\n",
            "        [ 0.3102, -0.3942, -0.2030,  0.0883, -0.4741],\n",
            "        [-0.1592, -0.0777, -0.3946, -0.2128,  0.2675],\n",
            "        [ 0.0611, -0.1933, -0.4350,  0.2928, -0.1087]])\n",
            "* Test k_means_quantize()\n",
            "    target bitwidth: 2 bits\n",
            "        num unique values before k-means quantization: 25\n",
            "        num unique values after  k-means quantization: 4\n",
            "* Test passed.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x1200 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAGjCAYAAACxJoNIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACDq0lEQVR4nO3dd1RUx98G8GcBaUvvoAgqKk0lihKNXQSNDcVuLGgSk6BRUaMmsSUqYjSWWLB3I4qisWEv0R92SSyxxq6AiIAsuCB73z94Wd2wKHJ3Kfp8zrnnsLMzszPDLfu9d+5diSAIAoiIiIiIiETQKe0GEBERERFR+cfAgoiIiIiIRGNgQUREREREojGwICIiIiIi0RhYEBERERGRaAwsiIiIiIhINAYWREREREQkGgMLIiIiIiISjYEFERERERGJxsCCSs2qVasgkUhw586ddy575MgRSCQSHDlyROPtep1EIsGkSZO0+hlE9OGSSCQYMmTIW/OJ2V8CwJ07dyCRSDBz5sxilafyJ/9/vmrVqhL93AEDBsDV1bVEP5PKDgYWRCI9evQIkyZNQnx8fGk3hYhK0JkzZzBkyBB4eXlBKpWicuXK6N69O65fv14in79w4cIS/9JIZc+GDRswZ86c0m7GO+P6+35iYEGlpm/fvsjKyoKLi8s7l23atCmysrLQtGlTLbTs3Tx69AiTJ09mYEH0gYmIiMCWLVvQqlUrzJ07F19++SWOHTuGunXr4tKlSxr9LHX7S34xI6DwwMLFxQVZWVno27dvyTeqCLj+vp/0SrsB9OGRyWSQSqXQ1dWFrq5userQ0dGBoaGhhltGL1++hEKhgL6+fmk3hajMCwsLw4YNG1S2lx49eqBWrVqYPn061q1bp7HPErO/pA+TRCLhcbKEZGZmwtjYuLSbUSbwigUV24ULF9C2bVuYmZnBxMQErVq1wsmTJ1Xy5M8LPnr0KL755hvY2dmhUqVKKu+9PmdYoVBg0qRJcHJygrGxMVq0aIErV67A1dUVAwYMUOZTd49F8+bN4e3tjStXrqBFixYwNjZGxYoVMWPGDJU2ZWdnY8KECahXrx7Mzc0hlUrRpEkTHD58+J3H4MiRI6hfvz4AICQkBBKJpMCc1lOnTqFNmzYwNzeHsbExmjVrhhMnTqjUM2nSJEgkEty8eRMDBgyAhYUFzM3NERISgszMTJW8+/fvR+PGjWFhYQETExPUrFkT33//vUqepKQkDBo0CPb29jA0NESdOnWwevVqlTyvz7meM2cOqlWrBgMDA1y5cuWdx4HoQ9SoUaMCQXj16tXh5eWFf/75553qWr9+PWrWrAlDQ0PUq1cPx44dU3n/v/tLV1dXXL58GUePHlXud5o3b/5OnykIAr788kvo6+tj69atb8ybv4+6fv06PvvsM5ibm8PW1hbjx4+HIAi4f/8+OnXqBDMzMzg4OGDWrFkF6pDL5Zg4cSLc3NxgYGAAZ2dnfPfdd5DL5Sr5Vq5ciZYtW8LOzg4GBgbw9PTEokWLCtTn6uqK9u3b4/jx42jQoAEMDQ1RtWpVrFmzRiVfTk4OJk+ejOrVq8PQ0BDW1tZo3Lgx9u/f/9Yxunz5Mlq2bAkjIyNUqlQJU6ZMwYoVKwocuwq7H++/x66UlBSMGjUKtWrVgomJCczMzNC2bVv89ddfKuXyj3GbNm3C1KlTUalSJRgaGqJVq1a4efOmMl/z5s2xa9cu3L17V7ke5N/f8N97LPLrVLf8956IPXv2oEmTJpBKpTA1NUW7du1w+fLlAv3btm0bvL29YWhoCG9vb8TExLx1TPPH5U3rb2pqKoYPHw5nZ2cYGBjAzc0NERERUCgUyjyvH8OWLFmiPIbVr18fZ86cUfm8hIQEhISEoFKlSjAwMICjoyM6depU4J6lhQsXwsvLCwYGBnByckJoaChSU1NV8uR/1zh37hyaNm0KY2PjAsfgDxmvWFCxXL58GU2aNIGZmRm+++47VKhQAYsXL0bz5s1x9OhR+Pn5qeT/5ptvYGtriwkTJkAmkxVa77hx4zBjxgx06NABgYGB+OuvvxAYGIgXL14UqV3Pnj1DmzZt0KVLF3Tv3h3R0dEYM2YMatWqhbZt2wIA0tPTsWzZMvTq1QtffPEFnj9/juXLlyMwMBCnT5+Gj49PkcfBw8MDP/30EyZMmIAvv/wSTZo0AZD3hQMADh06hLZt26JevXqYOHEidHR0lAfNP//8Ew0aNFCpr3v37qhSpQrCw8Nx/vx5LFu2DHZ2doiIiACQN+7t27dH7dq18dNPP8HAwAA3b95UCVSysrLQvHlz3Lx5E0OGDEGVKlWwefNmDBgwAKmpqRg2bJjKZ65cuRIvXrzAl19+CQMDA1hZWRW5/0SkShAEJCYmwsvLq8hljh49iqioKHz77bcwMDDAwoUL0aZNG5w+fRre3t5qy8yZMwdDhw6FiYkJfvjhBwCAvb19kT8zNzcXAwcORFRUFGJiYtCuXbsilevRowc8PDwwffp07Nq1C1OmTIGVlRUWL16Mli1bIiIiAuvXr8eoUaNQv3595XRVhUKBjh074vjx4/jyyy/h4eGBixcvYvbs2bh+/Tq2bdum/IxFixbBy8sLHTt2hJ6eHnbs2IFvvvkGCoUCoaGhKu25efMmunbtikGDBqF///5YsWIFBgwYgHr16in/B5MmTUJ4eDg+//xzNGjQAOnp6Th79izOnz+P1q1bF9rXhIQEtGjRAi9fvsTYsWMhlUqxZMkSGBkZFXmc/+vff//Ftm3b0K1bN1SpUgWJiYlYvHgxmjVrhitXrsDJyUkl//Tp06Gjo4NRo0YhLS0NM2bMQJ8+fXDq1CkAwA8//IC0tDQ8ePAAs2fPBgCYmJio/WwPDw+sXbtWJS01NRVhYWGws7NTpq1duxb9+/dHYGAgIiIikJmZiUWLFqFx48a4cOGCMgjZt28fgoOD4enpifDwcDx9+lT55f1t3rT+ZmZmolmzZnj48CEGDx6MypUr43//+x/GjRuHx48fF5j2tWHDBjx//hyDBw+GRCLBjBkz0KVLF/z777+oUKECACA4OBiXL1/G0KFD4erqiqSkJOzfvx/37t1T9mfSpEmYPHky/P398fXXX+PatWtYtGgRzpw5gxMnTijrAoCnT5+ibdu26NmzJz777LN32vbeewJRMQQFBQn6+vrCrVu3lGmPHj0STE1NhaZNmyrTVq5cKQAQGjduLLx8+VKljvz3bt++LQiCICQkJAh6enpCUFCQSr5JkyYJAIT+/fsr0w4fPiwAEA4fPqxMa9asmQBAWLNmjTJNLpcLDg4OQnBwsDLt5cuXglwuV/mMZ8+eCfb29sLAgQNV0gEIEydOfONYnDlzRgAgrFy5UiVdoVAI1atXFwIDAwWFQqFMz8zMFKpUqSK0bt1amTZx4kQBQIHP79y5s2Btba18PXv2bAGA8OTJk0LbM2fOHAGAsG7dOmVadna20LBhQ8HExERIT08XBEEQbt++LQAQzMzMhKSkpDf2kYiKZu3atQIAYfny5UXKD0AAIJw9e1aZdvfuXcHQ0FDo3LmzMu2/+0tBEAQvLy+hWbNmRfqc/O39l19+EXJycoQePXoIRkZGwt69e4tUPn8f9eWXXyrTXr58KVSqVEmQSCTC9OnTlenPnj0TjIyMVPbZa9euFXR0dIQ///xTpd7IyEgBgHDixAllWmZmZoHPDwwMFKpWraqS5uLiIgAQjh07pkxLSkoSDAwMhJEjRyrT6tSpI7Rr165I/Xzd8OHDBQDCqVOnVOo3Nzcv8L8o7Fjh4uKiMg4vXrwQcnNzVfLcvn1bMDAwEH766SdlWv4xzsPDQ+V4NXfuXAGAcPHiRWVau3btBBcXlwKfnf8//++xKZ9CoRDat28vmJiYCJcvXxYEQRCeP38uWFhYCF988YVK3oSEBMHc3Fwl3cfHR3B0dBRSU1OVafv27RMAqG3PfxW2/v7888+CVCoVrl+/rpI+duxYQVdXV7h3755K/6ytrYWUlBRlvu3btwsAhB07dgiCkLc+5q/7hUlKShL09fWFgIAAlf/P/PnzBQDCihUrlGn53zUiIyPf2scPEadC0TvLzc3Fvn37EBQUhKpVqyrTHR0d0bt3bxw/fhzp6ekqZb744ou3zg8+ePAgXr58iW+++UYlfejQoUVum4mJCT777DPla319fTRo0AD//vuvMk1XV1c5fUGhUCAlJQUvX76Er68vzp8/X+TPepv4+HjcuHEDvXv3xtOnT5GcnIzk5GTIZDK0atUKx44dU7msCwBfffWVyusmTZrg6dOnyvG0sLAAAGzfvr1A2Xy7d++Gg4MDevXqpUyrUKECvv32W2RkZODo0aMq+YODg2Frayu2u0QfvKtXryI0NBQNGzZE//79i1yuYcOGqFevnvJ15cqV0alTJ+zduxe5ubkabWN2dja6deuGnTt3Yvfu3QgICHin8p9//rnyb11dXfj6+kIQBAwaNEiZbmFhgZo1a6rsdzdv3gwPDw+4u7sr94XJyclo2bIlAKhMRX39ikBaWhqSk5PRrFkz/Pvvv0hLS1Npj6enp/JKMQDY2toW+GwLCwtcvnwZN27ceKe+7t69Gx9//LHKlWVbW1v06dPnnep5nYGBAXR08r565ebm4unTp8opreqOPyEhISrT7fL7+nr/iuvnn3/Gzp07sWrVKnh6egLIm2qbmpqKXr16qfyfdHV14efnp/w/PX78GPHx8ejfvz/Mzc2VdbZu3VpZV3Ft3rwZTZo0gaWlpUob/P39kZubW2CaYI8ePWBpaal8/d8xMjIygr6+Po4cOYJnz56p/cwDBw4gOzsbw4cPV/5/gLzvLmZmZti1a5dKfgMDA4SEhIjq5/uKU6HonT158gSZmZmoWbNmgfc8PDygUChw//59lakAVapUeWu9d+/eBQC4ubmppFtZWansNN6kUqVKkEgkKmmWlpb4+++/VdJWr16NWbNm4erVq8jJyXmndhZV/kHsTV8w0tLSVPpWuXJllffz33v27BnMzMzQo0cPLFu2DJ9//jnGjh2LVq1aoUuXLujatatyZ3j37l1Ur15dZecI5P1v8t9/nSb7TPShSkhIQLt27WBubo7o6GiVEylpaWnIyspSvtbX11eZcli9evUC9dWoUQOZmZl48uQJHBwcNNbO8PBwZGRkYM+ePQXuycjNzcWTJ09U0qysrFS+2P53H2Vubg5DQ0PY2NgUSH/69Kny9Y0bN/DPP/8UehIjKSlJ+feJEycwceJExMXFFbjHLC0tTeWL7H/bA+TtN1//AvnTTz+hU6dOqFGjBry9vdGmTRv07dsXtWvXVtuWfHfv3i0wrReA2mNfUSkUCsydOxcLFy7E7du3VQJHa2vrAvnfdEwQIzY2FpMnT8a4ceMQHBysTM8/buUHfP9lZmYG4NVxRN26W1iQVFQ3btzA33//XaR1BXj7GBkYGCAiIgIjR46Evb09Pv74Y7Rv3x79+vVTblv5/fnv/1ZfXx9Vq1YtcNysWLEiH3JSCAYWVCLEzEl9F4VdFREEQfn3unXrMGDAAAQFBWH06NGws7ODrq4uwsPDcevWLY21Jf+Kwi+//FLofRv/nQv7tvYbGRnh2LFjOHz4MHbt2oXY2FhERUWhZcuW2LdvX7GeGlNS/xui91VaWhratm2L1NRU/PnnnwXmyQ8bNkzl4QnNmjXT+o97FiYwMBCxsbGYMWMGmjdvrvLUoPv37xc40XD48GGVAETdPqYo+12FQoFatWrh119/VZvX2dkZAHDr1i20atUK7u7u+PXXX+Hs7Ax9fX3s3r0bs2fPLnCltiif3bRpU9y6dQvbt2/Hvn37sGzZMsyePRuRkZEqV2C04b9XnKZNm4bx48dj4MCB+Pnnn2FlZQUdHR0MHz5c7VXoovTvXd2+fRt9+vRB69atMWXKFJX38tuwdu1atQGtnp72vzYqFAq0bt0a3333ndr3a9SoofK6KGM0fPhwdOjQAdu2bcPevXsxfvx4hIeH49ChQ/joo4/euY08bhaOgQW9M1tbWxgbG+PatWsF3rt69Sp0dHSUB4l3kf989ps3b6oc3J4+fSr67MzroqOjUbVqVWzdulXl6sbEiROLVd9/r5Dkq1atGoC8Mzz+/v7FqlsdHR0dtGrVCq1atcKvv/6KadOm4YcffsDhw4fh7+8PFxcX/P3331AoFCpXLa5evQoAxfrdECJS78WLF+jQoQOuX7+OAwcOqJ0G8t1336lM0fzvFVh1U3SuX78OY2PjN05TLGzf8yYff/wxvvrqK7Rv3x7dunVDTEyM8suig4NDgScl1alT550/Q51q1arhr7/+QqtWrd7Y7h07dkAul+OPP/5QORNdnKf2vc7KygohISEICQlBRkYGmjZtikmTJr0xsHBxcVH7v1F37LO0tCzw9KDs7Gw8fvxYJS06OhotWrTA8uXLVdJTU1MLXPUpqndZD7KystClSxdYWFjg999/L3BlO/+4ZWdn98bjVv5xpKjjo86bjp0ZGRkaPW7m1zty5EiMHDkSN27cgI+PD2bNmoV169Yp+3Pt2jWVKd7Z2dm4ffu2xtvyPuM9FvTOdHV1ERAQgO3bt6s8qi0xMREbNmxA48aNlZdL30WrVq2gp6dX4LGC8+fPF9tkFflnN14/m3Hq1CnExcUVqz6pVAoABQ4q9erVQ7Vq1TBz5kxkZGQUKPffKQdFkZKSUiAt/2pI/iMbP/30UyQkJCAqKkqZ5+XLl/jtt99gYmKCZs2avfPnElFBubm56NGjB+Li4rB582Y0bNhQbT5PT0/4+/srl9fvpwCAuLg4lakj9+/fx/bt2xEQEPDGq5BSqbTAfqco/P39sXHjRsTGxqJv377Ks9SGhoYq7fT39y/yNNS36d69Ox4+fIilS5cWeC8rK0v5tEB1++e0tDSsXLmy2J/9+pQsIO9KsZubW4HH3P7Xp59+ipMnT+L06dPKtCdPnmD9+vUF8larVq3A3P8lS5YUuGKhq6tb4GrD5s2b8fDhwyL1RR2pVFrg3pPCfPXVV7h+/TpiYmLU/m8DAwNhZmaGadOmqUwTzpd/3HJ0dISPjw9Wr16t8tn79+8v8iPLC1t/u3fvjri4OOzdu7fAe6mpqXj58mWR6s+XmZlZ4MmS1apVg6mpqXId8Pf3h76+PubNm6fy/1m+fDnS0tKK/NQ04hULKqYpU6Yof0/hm2++gZ6eHhYvXgy5XF7gdyOKyt7eHsOGDcOsWbPQsWNHtGnTBn/99Rf27NkDGxubYp2dU6d9+/bYunUrOnfujHbt2uH27duIjIyEp6en2gDgbapVqwYLCwtERkbC1NQUUqkUfn5+qFKlCpYtW4a2bdvCy8sLISEhqFixIh4+fIjDhw/DzMwMO3bseKfP+umnn3Ds2DG0a9cOLi4uSEpKwsKFC1GpUiU0btwYAPDll19i8eLFGDBgAM6dOwdXV1dER0fjxIkTmDNnDkxNTd+5j0RU0MiRI/HHH3+gQ4cOSElJKfCDeK9fpXgTb29vBAYGqjxuFgAmT578xnL16tXDokWLMGXKFLi5ucHOzq7QufH/FRQUhJUrV6Jfv34wMzPD4sWLi1SuuPr27YtNmzbhq6++wuHDh/HJJ58gNzcXV69exaZNm7B37174+voiICAA+vr66NChAwYPHoyMjAwsXboUdnZ2Bc7+F5WnpyeaN2+OevXqwcrKCmfPnkV0dDSGDBnyxnLfffcd1q5dizZt2mDYsGHKx83mXxV+3eeff46vvvoKwcHBaN26Nf766y/s3bu3wFWI9u3b46effkJISAgaNWqEixcvYv369Spnyd9VvXr1EBUVhbCwMNSvXx8mJibo0KFDgXy7du3CmjVrEBwcjL///lulDyYmJggKCoKZmRkWLVqEvn37om7duujZsydsbW1x79497Nq1C5988onyZF94eDjatWuHxo0bY+DAgUhJScFvv/0GLy+vIh1LC1t/R48ejT/++APt27dXPjpYJpPh4sWLiI6Oxp07d97p6s7169fRqlUrdO/eHZ6entDT00NMTAwSExPRs2dPAHkzMcaNG4fJkyejTZs26NixI65du4aFCxeifv36Rd6WCXzcLBXf+fPnhcDAQMHExEQwNjYWWrRoIfzvf/9TyZP/iMQzZ84UKK/u8YkvX74Uxo8fLzg4OAhGRkZCy5YthX/++UewtrYWvvrqK2W+wh436+XlVeBz+vfvr/LoO4VCIUybNk1wcXERDAwMhI8++kjYuXNngXyCULTHzQpC3uPtPD09BT09vQKP97tw4YLQpUsXwdraWjAwMBBcXFyE7t27CwcPHlTmyX+U438fI/vfMTp48KDQqVMnwcnJSdDX1xecnJyEXr16FXgsX2JiohASEiLY2NgI+vr6Qq1atQo8cvD1x08S0bvLf+xkYUtRABBCQ0OFdevWCdWrV1fuk17ftwmC+v1lQkKC0K5dO8HU1FQA8MZHzxa2vS9cuFAAIIwaNeqN7SxsH9W/f39BKpUWyK9uf5ydnS1EREQIXl5egoGBgWBpaSnUq1dPmDx5spCWlqbM98cffwi1a9cWDA0NBVdXVyEiIkJYsWJFgf67uLiofYxss2bNVMZiypQpQoMGDQQLCwvByMhIcHd3F6ZOnSpkZ2e/sc+CIAh///230KxZM8HQ0FCoWLGi8PPPPwvLly8v0Jbc3FxhzJgxgo2NjWBsbCwEBgYKN2/eVPu42ZEjRwqOjo6CkZGR8MknnwhxcXEF2px/jNu8ebNKe9Q9QjYjI0Po3bu3YGFhofKo1//mzV+H1C3/PfYdPnxYCAwMFMzNzQVDQ0OhWrVqwoABA1QeiywIgrBlyxbBw8NDMDAwEDw9PYWtW7eqPZaq86b19/nz58K4ceMENzc3QV9fX7CxsREaNWokzJw5U/l/e9Mx7PVjd3JyshAaGiq4u7sLUqlUMDc3F/z8/IRNmzYVKDd//nzB3d1dqFChgmBvby98/fXXwrNnz1TyFPZdg/JIBEHEHUBEJSA1NRWWlpaYMmWK8od0iIiISsOqVasQEhKC27dvF/jFaqIPHe+xoDLl9Ucy5sv/lc3/PhqRiIiIiMoO3mNBZUpUVBRWrVqFTz/9FCYmJjh+/Dh+//13BAQE4JNPPint5hERERFRIRhYUJlSu3Zt6OnpYcaMGUhPT1fe0P3fZ20TERERUdnCeyyIiIiIiEg03mNBRERERESiMbAgIiKNEgQB6enpBX4IjIiIyp932acX+x4LhUKBR48ewdTUVGM/XEZERCVPEAQ8f/4cpqamMDMzE71PT09Ph4WFBe7fvw8zMzMNtZKIiEpDeno6nJ2dkZqaCnNz8zfmLXZg8ejRIzg7Oxe3OBERlUFpaWmig4Hnz58DAI8RRETvkefPn2svsDA1NQUAnpEiIirn8s9G3b9/X7lvFyO/jpk7TsJIaiK6vvfZ1Zylpd2EciPEdlhpN6HciEvNLO0mlAvOGf+WdhPKhczMTPTs2bNIx4diBxb5l8rNzMwYWBARvQc0MQ0KeHV8MJKawMhEfKDyPjPINijtJpQbJlyXiszopW5pN6FckArS0m5CuVKU4wNv3iYiIiIiItEYWBARERERkWgMLIiIiIiISDQGFkREREREJBoDCyIiIiIiEo2BBRERERERicbAgoiIiIiIRGNgQUREREREojGwICIiIiIi0RhYEBERERGRaAwsiIiIiIhINAYWREREREQkGgMLIiIiIiISjYEFERERERGJxsCCiIiIiIhEY2BBRERERESiMbAgIiIiIiLRGFgQEREREZFoDCyIiIiIiEg0BhZERERERCQaAwsiIiIiIhKNgQUREREREYnGwIKIiIiIiERjYEFERERERKIxsCAiIiIiItEYWBARERERkWgMLIiIiIiISDQGFkREREREJBoDCyIiIiIiEq3MBxaCIGDChAlwdHSEkZER/P39cePGjTeWWbRoEWrXrg0zMzOYmZmhYcOG2LNnj/L9O3fuQCKRqF02b96s7S5pxYIFC+Dq6gpDQ0P4+fnh9OnTb8y/efNmuLu7w9DQELVq1cLu3btV3k9MTMSAAQPg5OQEY2NjtGnT5q3jXl68y1ht3boVvr6+sLCwgFQqhY+PD9auXat8PycnB2PGjEGtWrUglUrh5OSEfv364dGjRyXRFa0rzvb3uunTp0MikWD48OEq6bdu3ULnzp1ha2sLMzMzdO/eHYmJiRpufcl51+0v38aNGyGRSBAUFFRonq+++goSiQRz5szRTGPfM4IgIGbxLIxo64vBTWrgl9DeSLx3+41lDkevxYTegfimhRe+aeGFqQOD8Pf/DqvkifiqBwY2cFFZ1oR/r82uaJUgCDi+7E8s6rQAc1r+ik3DovDsfspby13Ych5LukZidstZWPfFWjy+8lj5XlZ6Fg7OPoDlvZZiTstfsbjLIhyccwDyDLk2u6JV69YtR4sWdeHtXQlduwbir7/OF5o3KmotevVqD19fN/j6uqF//+AC+WvUsFW7LFs2X9td0Tpue0Wzbds29O7dG23atEFoaCiuXr1apHKHDh1Cq1atMH78+ELzzJ49G61atcKWLVs01VzRynxgMWPGDMybNw+RkZE4deoUpFIpAgMD8eLFi0LLVKpUCdOnT8e5c+dw9uxZtGzZEp06dcLly5cBAM7Oznj8+LHKMnnyZJiYmKBt27Yl1TWNiYqKQlhYGCZOnIjz58+jTp06CAwMRFJSktr8//vf/9CrVy8MGjQIFy5cQFBQEIKCgnDp0iUAeTuLoKAg/Pvvv9i+fTsuXLgAFxcX+Pv7QyaTlWTXNO5dx8rKygo//PAD4uLi8PfffyMkJAQhISHYu3cvACAzMxPnz5/H+PHjcf78eWzduhXXrl1Dx44dS7JbWlOc7S/fmTNnsHjxYtSuXVslXSaTISAgABKJBIcOHcKJEyeQnZ2NDh06QKFQaKsrWvOu61S+O3fuYNSoUWjSpEmheWJiYnDy5Ek4OTlputnvjT1rInEgahX6jZ2GH1dsh4GRMWZ92xc58sLXUUt7R3QNHYOJq3diwqodcPdthN9GfYGHt66r5Gsa1Auzd59RLt2GjtN2d7Tm9PrTuBB9Hq1HBaDPks9QwagCosM246X8ZaFlrh78B0fmH0bDkE/Qd3l/2LnZIjpsE2TP8o4DGckZyEjOQPPQFhiwNgRtf/gUd07eRuz0PYXWWZbt2hWD8PAJGDJkFLZtOwh3dy8MGtQdT58+UZv/9OkTaN++C9asiUFU1B44Ojph4MBuSEh4FXydOHFJZQkPnwuJRIKAgPYl1S2t4bb3docPH0ZkZCT69euHyMhIVKtWDWPGjMGzZ8/eWC4hIQGLFy9GrVq1Cs1z/Phx/PPPP7C2ttZ0s0Up04GFIAiYM2cOfvzxR3Tq1Am1a9fGmjVr8OjRI2zbtq3Qch06dMCnn36K6tWro0aNGpg6dSpMTExw8uRJAICuri4cHBxUlpiYGHTv3h0mJiYl1DvN+fXXX/HFF18gJCQEnp6eiIyMhLGxMVasWKE2/9y5c9GmTRuMHj0aHh4e+Pnnn1G3bl3Mn593BuXGjRs4efIkFi1ahPr166NmzZpYtGgRsrKy8Pvvv5dk1zTuXceqefPm6Ny5Mzw8PFCtWjUMGzYMtWvXxvHjxwEA5ubm2L9/P7p3746aNWvi448/xvz583Hu3Dncu3evJLumccXd/gAgIyMDffr0wdKlS2Fpaany3okTJ3Dnzh2sWrUKtWrVQq1atbB69WqcPXsWhw4d0mKPtONd1ykAyM3NRZ8+fTB58mRUrVpVbZ6HDx9i6NChWL9+PSpUqKCt5pdrgiBg/8bl6DBwCD5qFgDn6h74fNKvSE1Owvmj+wot59PEH7U/aQn7ylXg4FIVwd98B0NjY9y6pHq2Wd/QCOY2dsrFyMRU213SCkEQcH7zWXzcryHcmlSHrZsdPv2xHTKeZuDmn4VfgTy78SxqdaiNWu1qwaaKDVqPDkQFwwq4tPMiAMC2qi06TQ1CtcZusKhoicr1XND4yyb498QtKF6Wv5MEK1dGonv3zxAc3BtubjXx008zYWhohOjoDWrzz5oViT59BsLTsxaqVauOqVPnQKFQIC7umDKPra29ynLgQCz8/BqjcmXXEuqVdnDbK5ro6Gh8+umnaNOmDVxdXTF8+HAYGBggNja20DK5ubmYNm0a+vfvD0dHR7V5njx5gt9++w3ff/899PT0tNX8YinTgcXt27eRkJAAf39/ZZq5uTn8/PwQFxdXpDpyc3OxceNGyGQyNGzYUG2ec+fOIT4+HoMGDdJIu0tSdnY2zp07pzJGOjo68Pf3L3SM4uLiVPIDQGBgoDK/XJ53GdvQ0FClTgMDA+UX6vKoOGP1OkEQcPDgQVy7dg1NmzYtNF9aWhokEgksLCw00exSI2b7Cw0NRbt27QqsZ0De+iWRSGBgYKBMMzQ0hI6OTrlbv4q7Tv3000+ws7MrdJ+jUCjQt29fjB49Gl5eXhpv9/viyaP7SHv6BJ4NGivTjE3MUNXLB7cuFj6F5XWK3Fyc2vcH5FlZqFarrsp7J2O34dvWPhjfszWiF0RA/iJLo+0vKWmP0iB7KoNLfRdlmoGJARw9HfHokvppm7k5uUi8ngAXX1dlmkRHgsq+Lnh0ufCpnnKZHPpSfejolemvFwVkZ2fj8uW/0KhRM2Wajo4OGjVqivj4s0WqIysrCy9fvoSFhaXa95OTk3D06H5069ZHI20uTdz23i4nJwfXr19H3bqv+qajo4O6deviypUrhZZbu3YtLCws8Omnn6p9X6FQYPr06ejevTtcXV013WzRihzmyOVy5RdOAEhPT9dKg16XkJAAALC3t1dJt7e3V75XmIsXL6Jhw4Z48eIFTExMEBMTA09PT7V5ly9fDg8PDzRq1EgzDS9BycnJyM3NVTtGhc3jS0hIeOOYuru7o3Llyhg3bhwWL14MqVSK2bNn48GDB3j8+LG6KsuF4owVkBcoVKxYEXK5HLq6uli4cCFat26tNu+LFy8wZswY9OrVC2ZmZhptf0kr7va3ceNGnD9/HmfOnFH7/scffwypVIoxY8Zg2rRpEAQBY8eORW5ubrlbv4qzTh0/fhzLly9HfHx8ofVGRERAT08P3377rSabqzWlcXwAgPSnedPNzKxsVNLNrGyQVsj0lXwPbl7F1EGdkZMth4GRFENmLEbFqjWU7/sFdoKNQ0VY2Nrj/s1/ED1/OhLu3sKQGUs03xEtk6XkTV0ytpSqpBtbSiFLyVBbJistE0KuAKmVsUq61EqKlLvq783ITM1E3Ko41O5QRwOtLlnPnqUgNzcXNja2Kuk2Nnb499+bRapj5syfYGfngEaN1J94iomJglRqgoCAdqLbW9q47b1dWloaFApFgav2lpaWuH//vtoyFy9exJ49e7BkSeF93bhxI3R1ddGlSxeNtldTinxKITw8HObm5srF2dlZ441Zv349TExMlEtOTk6x66pZsybi4+Nx6tQpfP311+jfv7/aCDErKwsbNmwol1crtKVChQrYunUrrl+/DisrKxgbG+Pw4cNo27YtdHTK11koTTA1NUV8fDzOnDmDqVOnIiwsDEeOHCmQLycnB927d4cgCFi0aFHJN1QkTWx/9+/fx7Bhw7B+/XqVK16vs7W1xebNm7Fjxw6YmJjA3NwcqampqFu37nu/fj1//hx9+/bF0qVLYWNjozbPuXPnMHfuXKxatQoSiaSEW1g8JXF8AIC42Bh83cxDueS+LPz+gLdxcKmKSev24McV29Ei+DMsmzwSD/99Nc+7eefe8G7YDJXc3NGwTWd8PvFXnD+yF0kP7mqiK1p1Zd9lzG09W7koXuZq/TPlMjm2jt4Ca1drNBr0idY/r6xZvHgudu2KwYIFq2BgoH7fFx29AR06BBf6flnGbU/7MjMzMX36dISFhcHc3FxtnuvXr2Pr1q347rvvyuzxochXLMaNG4ewsDDl6/T0dI0fPDp27Ag/Pz/l6/wzYImJiSrzzBITE+Hj4/PGuvT19eHm5gYAqFevHs6cOYO5c+di8eLFKvmio6ORmZmJfv36aagXJcvGxga6uroFnqiTmJgIBwcHtWUcHBzemr9evXqIj49HWloasrOzYWtrCz8/P/j6+mq+EyWkOGMF5F26zF+XfHx88M8//yA8PBzNmzdX5skPKu7evYtDhw6Vy6sVmtj+zp07h6SkJJVLv7m5uTh27Bjmz5+vvOoTEBCAW7duITk5GXp6erCwsICDg0Oh9xuUVe+6Tt26dQt37txBhw4dlGn5N6zr6enh2rVr+PPPP5GUlITKlSsr8+Tm5mLkyJGYM2cO7ty5o53OiFASxwcA8GnSGlW9PlK+fpmdnfd5KcmwsHl11Sg9JRmVa6i/Qp1Pr4I+7J1dAQCuHrVw+8pfOBC1Ev3HhavNX9U773OT7t+BXSUXtXnKCrfGbnD0fHXDf252XmCR+UwGE5tX9xFmPpPBzs2+QHkAMDI3hkRXAllKpkq6LEUGqbXqlY/sTDm2jNwMfWN9BE3rDF09XU11pcRYWlpBV1cXycmqZ9uTk5Nga2v3xrLLly/AkiXzsGrVFri7q5+6eOZMHG7fvok5c5ZqrM0lidveuzM3N4eOjk6BG7WfPXsGKyurAvkfPXqEhIQE/Pjjj8o0QRAAAK1bt8bq1atx8eJFpKamolevXso8CoUCkZGR2LJlCzZsUH8/UEkqcmBhYGCgMidaG0xNTWFq+uoGHUEQ4ODggIMHDyq/yKSnpyuvQrwLhUKhcqk+3/Lly9GxY0fY2tqqKVX26evro169ejh48KDykZUKhQIHDx7EkCFD1JZp2LAhDh48qPII0P3796u9ByU/ar5x4wbOnj2Ln3/+WeN9KCnFGSt1/rsu5QcVN27cwOHDh8vcExqKShPbX6tWrXDx4kWVtJCQELi7u2PMmDHQ1VX9wpF/1v7QoUNISkoqd0/Tetd1yt3dvcD4/Pjjj3j+/Dnmzp0LZ2dn9O3bV+09UH379kVISIjW+iJGSRwfAMBIagIj6asvxoIgwNzaFlfOnEDlGnlf6LIynuPfy/FoEfzZO9UtKBTKL0vq3Lue91RBc5s3f8ksC/SNDaBv/Or/IQgCpNZS3D17F3bV874EymVyPL7yGD5BH6mtQ7eCLuxrOODeubuo3rR6Xj0KAffO3cVHXV6dOJDL5IgO2wzdCrroHNEFegZl60bSotLX14eXVx3ExR1D69Z5c9vzbsT+E599VviMhqVLf8OiRbOxYsUm1KrlU2i+6Oj18PauAw8Pb003vURw23t3FSpUQI0aNXDhwgU0bpx3L4pCoVA+jfO/KleujGXLlqmkrVixAllZWQgNDYWtrS38/f1VTtwBwJgxY9C6dWu0adNGa315F2V6D5D//PspU6agevXqqFKlCsaPHw8nJyeVf0qrVq3QuXNn5YF83LhxaNu2LSpXroznz59jw4YNOHLkiPIRoflu3ryJY8eOFfgNh/ImLCwM/fv3h6+vLxo0aIA5c+ZAJpMpv4T069cPFStWRHh43tmAYcOGoVmzZpg1axbatWuHjRs34uzZsypz+jZv3gxbW1tUrlwZFy9exLBhwxAUFISAgIBS6aOmvOtYhYeHw9fXF9WqVYNcLsfu3buxdu1a5VSnnJwcdO3aFefPn8fOnTuRm5urvP/AysoK+vr6pdNRDSjO9mdqagpvb9UDp1QqhbW1tUr6ypUr4eHhAVtbW8TFxWHYsGEYMWIEatasWVLd05h3WacMDQ0LjE/+Tf756dbW1gWC0woVKsDBwaFcjo82SSQStO45CDtX/AZ75yqwdXJGTOQsWNjYoW6zV/uqX77phbrNA9Gq+wAAQPSCCNRq2BzWDk54kSnDyb3bce38SYTNy/uNmqQHd3Fy7zbUbtQSJuYWuH/zKjbO/gk1PvKDc3WP0uiqKBKJBHW7+eLk6jhYOlvC3NECJ5b9CRNrE7g1qa7Mt2nYRrg1rYG6wXlfXHx7+mLP1N2wd3eAo4cjzm06i5ysHHi3y3sEplwmR/SITciRv0S7Ce2QLZMjW5Z30sXIwhg6uuVramNIyFcYM2YovL19ULt2XaxevRhZWZkIDs47Ozx6dCjs7R0walTe7wosWTIPc+dG4NdfI1GxojOePMm7cmlsLIX0tS/hGRnPERu7A2PHTi75TmkJt72i6dq1KyIiIlCjRg24u7tjy5YtePHiBQIDAwHk/daTjY0NPv/8c+jr66NKlSoq5fOfVJqfnj/d9HV6enqwsrLS2hTUd1WmAwsA+O677yCTyfDll18iNTUVjRs3RmxsrMr87fwpFfmSkpLQr18/PH78GObm5qhduzb27t1b4IbbFStWoFKlSuX+y3KPHj3w5MkTTJgwAQkJCfDx8UFsbKzyhtJ79+6pzF1v1KgRNmzYgB9//BHff/89qlevjm3btql84Xn8+DHCwsKU02D69ev3xh9pKS/edaxkMhm++eYbPHjwAEZGRnB3d8e6devQo0cPAHmPBP3jjz8AoMD0oMOHD6tMlyqPirP9FcW1a9cwbtw4pKSkwNXVFT/88ANGjBih6eaXiHddp0iz2vb7CvIXmVg9bRwyM9JRvY4vwuauQYXX5rEnPbyH56mvpiOkpyRj2eQwpCUnwcjEFJXc3BE2by28/PJ+U0SvQgVcOX0C+39fAfmLLFjZO6Jei7boMHBoifdPUxr0aYCcF9nYN2Mf5BkvULFWJQTP6qZyhSH1YSqyUl9NfXJv5YHM1CycWHYcmSky2LrZoeusbpBa5U2FSryWqPzBvGU9VKf4fLF5MMwd1c8TL6vateuMlJSnmDcvAk+eJMHDwxvLl0fB5v/PlD9+/AA6Oq/mtf/++yrk5GRj6NCBKvUMGTIa3377nfL1zp0xEAQB7duXzZtti4vb3tu1aNECaWlpWLVqFZ49e4Zq1aph+vTpyqlQSUlJZfZeieKSCPkTuN5Reno6zM3NkZaWVi7nkxMRUR5N78/z61tw6FK5ff58SbmSvaC0m1BufGk3urSbUG4cT818eyaCy/OiPfHrQyeTydCxY8ciHSN4Go2IiIiIiERjYEFERERERKIxsCAiIiIiItEYWBARERERkWgMLIiIiIiISDQGFkREREREJBoDCyIiIiIiEo2BBRERERERicbAgoiIiIiIRGNgQUREREREojGwICIiIiIi0RhYEBERERGRaAwsiIiIiIhINAYWREREREQkGgMLIiIiIiISjYEFERERERGJxsCCiIiIiIhEY2BBRERERESiMbAgIiIiIiLRGFgQEREREZFoDCyIiIiIiEg0BhZERERERCQaAwsiIiIiIhKNgQUREREREYnGwIKIiIiIiERjYEFERERERKIxsCAiIiIiItEYWBARERERkWgMLIiIiIiISDQGFkREREREJJpeaTeAiIjeT1dzlsIg26C0m1GmfWk3urSbUG4cT80s7SbQe8bZuXZpN6FcyMh4XuS8vGJBRERERESiMbAgIiIiIiLRGFgQEREREZFoDCyIiIiIiEg0BhZERERERCQaAwsiIiIiIhKNgQUREREREYnGwIKIiIiIiERjYEFERERERKIxsCAiIiIiItEYWBARERERkWgMLIiIiIiISDQGFkREREREJBoDCyIiIiIiEo2BBRERERERicbAgoiIiIiIRGNgQUREREREojGwICIiIiIi0RhYEBERERGRaAwsiIiIiIhINAYWREREREQkGgMLIiIiIiISjYEFERERERGJxsCCiIiIiIhEY2BBRERERESiMbAgIiIiIiLRGFgQEREREZFoDCyIiIiIiEg0BhZERERERCQaAwsiIiIiIhKNgQUREREREYnGwIKIiIiIiEQr84GFIAiYMGECHB0dYWRkBH9/f9y4ceOt5RYsWABXV1cYGhrCz88Pp0+fLrT+tm3bQiKRYNu2bRpufckpan8BYOnSpWjSpAksLS1haWkJf3//AvknTZoEd3d3SKVSZZ5Tp05puxslojjrVHh4OOrXrw9TU1PY2dkhKCgI165dU8nz4sULhIaGwtraGiYmJggODkZiYqI2u6J1xRmrSZMmQSKRqCzu7u4qed63sXqX7e/y5csIDg6Gq6srJBIJ5syZUyBPbm4uxo8fjypVqsDIyAjVqlXDzz//DEEQtNiL8kkQBBxf9icWdVqAOS1/xaZhUXh2P+Wt5S5sOY8lXSMxu+UsrPtiLR5feax8Lys9CwdnH8DyXksxp+WvWNxlEQ7OOQB5hlybXdGqdeuWo0WLuvD2roSuXQPx11/nC80bFbUWvXq1h6+vG3x93dC/f3CB/DVq2Kpdli2br+2uaJ0gCIhZPAsj2vpicJMa+CW0NxLv3X5jmcPRazGhdyC+aeGFb1p4YerAIPz9v8MqeSK+6oGBDVxUljXh32uzK1rFcSqaD23bK/OBxYwZMzBv3jxERkbi1KlTkEqlCAwMxIsXLwotExUVhbCwMEycOBHnz59HnTp1EBgYiKSkpAJ558yZA4lEos0uaN279BcAjhw5gl69euHw4cOIi4uDs7MzAgIC8PDhQ2WeGjVqYP78+bh48SKOHz8OV1dXBAQE4MmTJyXVLa0pzjp19OhRhIaG4uTJk9i/fz9ycnIQEBAAmUymzDNixAjs2LEDmzdvxtGjR/Ho0SN06dKlJLqkNcUZKwDw8vLC48ePlcvx48dV3n+fxupdt7/MzExUrVoV06dPh4ODg9o8ERERWLRoEebPn49//vkHERERmDFjBn777TdtdqVcOr3+NC5En0frUQHos+QzVDCqgOiwzXgpf1lomasH/8GR+YfRMOQT9F3eH3ZutogO2wTZs7ztOSM5AxnJGWge2gID1oag7Q+f4s7J24idvqekuqVRu3bFIDx8AoYMGYVt2w7C3d0LgwZ1x9On6vfnp0+fQPv2XbBmTQyiovbA0dEJAwd2Q0LCq+DrxIlLKkt4+FxIJBIEBLQvqW5pzZ41kTgQtQr9xk7Djyu2w8DIGLO+7YsceeH7PUt7R3QNHYOJq3diwqodcPdthN9GfYGHt66r5Gsa1Auzd59RLt2GjtN2d7SG4/R2H+K2JxGKeQosPT0d5ubmSEtLg5mZmabbBSAvGnZycsLIkSMxatQoAEBaWhrs7e2xatUq9OzZU205Pz8/1K9fH/Pn50VvCoUCzs7OGDp0KMaOHavMFx8fj/bt2+Ps2bNwdHRETEwMgoKCtNIXbSpqfwuTm5sLS0tLzJ8/H/369VObJ///feDAAbRq1Uqj7S9JxV2n/uvJkyews7PD0aNH0bRpU6SlpcHW1hYbNmxA165dAQBXr16Fh4cH4uLi8PHHH2utT9pS3LGaNGkStm3bhvj4eLXvv29jJWb7c3V1xfDhwzF8+HCV9Pbt28Pe3h7Lly9XpgUHB8PIyAjr1q3TeB80vT/Pr2/o3mEwkBpooIXqCYKAyKCF8O1RH/V7NwAAyDPkWNhxPtp+/ync/T3Ullv3xVo4eDjAP6x1Xj0KAYu7LMJHwXXh11f9+nft0FXs/nkXhu0fAR09zZ2T+9JutMbqKkzXroGoVcsHEydGAMhbR5s2rYO+fT/H4MHD3lo+NzcXvr5umDBhOjp37qE2z9df94NMloE1a7ZqtO2vO56aqbW68wmCgLBP6yOwzxdo89lgAEBmRjqGt/HFoAkz4RfQsch1DfWvjW5Dv0fTTnn7yoivesC5hid6h03USttL0vsyTo0tjLVa//uy7WVkPEfdulWLdIwo01csbt++jYSEBPj7+yvTzM3N4efnh7i4OLVlsrOzce7cOZUyOjo68Pf3VymTmZmJ3r17Y8GCBYWeNSwPitrfN8nMzEROTg6srKwK/YwlS5bA3NwcderU0Ui7S0tx1il10tLSAEA5ZufOnUNOTo5Kve7u7qhcufI71VuWiBmrGzduwMnJCVWrVkWfPn1w79495Xvv01hpYvtTp1GjRjh48CCuX887i/fXX3/h+PHjaNu2reg2v0/SHqVB9lQGl/ouyjQDEwM4ejri0aVHasvk5uQi8XoCXHxdlWkSHQkq+7rg0WX1ZQBALpNDX6qv0aCiJGRnZ+Py5b/QqFEzZZqOjg4aNWqK+PizRaojKysLL1++hIWFpdr3k5OTcPTofnTr1kcjbS5NTx7dR9rTJ/Bs0FiZZmxihqpePrh1sfApLK9T5Obi1L4/IM/KQrVadVXeOxm7Dd+29sH4nq0RvSAC8hdZGm1/SeE4vd2Huu3pFTWjXC6HXP5qfml6erpWGvS6hIQEAIC9vb1Kur29vfK9/0pOTkZubq7aMlevXlW+HjFiBBo1aoROnTppuNUlq6j9fZMxY8bAyclJ5csRAOzcuRM9e/ZEZmYmHB0dsX//ftjY2Gis7aWhOOvUfykUCgwfPhyffPIJvL29lfXq6+vDwsKi2PWWNcUdKz8/P6xatQo1a9bE48ePMXnyZDRp0gSXLl2CqanpezVWmtj+1Bk7dizS09Ph7u4OXV1d5ObmYurUqejTp+wcPF5XGscHAJCl5E1dMraUqqQbW0ohS8lQWyYrLRNCrgCpleqZSqmVFCl31d+bkZmaibhVcajdofydWHn2LAW5ubmwsbFVSbexscO//94sUh0zZ/4EOzsHNGrUVO37MTFRkEpNEBDQTnR7S1v607wpjGZWqsc6MysbpBUyfSXfg5tXMXVQZ+Rky2FgJMWQGYtRsWoN5ft+gZ1g41ARFrb2uH/zH0TPn46Eu7cwZMYSzXdEyzhOb/ehbntFDizCw8MxefJkbbYF69evx+DBg5Wvd+3apZXP+eOPP3Do0CFcuHBBK/WXJ9OnT8fGjRtx5MgRGBoaqrzXokULxMfHIzk5GUuXLkX37t1x6tQp2NnZlVJr35021qnQ0FBcunSpwH0D5Z2mxur1s+q1a9eGn58fXFxcsGnTJgwaNEh0Oz8EmzZtwvr167FhwwZ4eXkhPj4ew4cPh5OTE/r371/azSugJI4PAHBl32Xs/2Wf8nWXGcFa/0y5TI6to7fA2tUajQZ9ovXPK2sWL56LXbtisHbtNhgYGKrNEx29AR06BBf6flkWFxujcmPw8Nkri12Xg0tVTFq3B1kZz3H20G4smzwSYyKjlF+am3furcxbyc0dFtZ2+CW0N5Ie3IVdJZfCqi0TOE4lr7xue0UOLMaNG4ewsDDl6/T0dDg7O2u0MR07doSfn5/ydf4ZsMTERDg6OirTExMT4ePjo7YOGxsb6OrqFnjCTGJionLK06FDh3Dr1q0CZ0yDg4PRpEkTHDlyRHxnSkhR+luYmTNnYvr06Thw4ABq165d4H2pVAo3Nze4ubnh448/RvXq1bF8+XKMG1d+bqLSxDr1uiFDhmDnzp04duwYKlWqpEx3cHBAdnY2UlNTVdarovwfygpNj1U+CwsL1KhRAzdv5p2heR/GKp+Y7e9NRo8ejbFjxyrvY6lVqxbu3r2L8PDwMhlYlMTxAQDcGrvB0dNJ+To3OxcAkPlMBhMbE2V65jMZ7NzsC5QHACNzY0h0JZClqM7Xl6XIILVWvfKRnSnHlpGboW+sj6BpnaGrp6uprpQYS0sr6OrqIjlZ9SxycnISbG3ffJJo+fIFWLJkHlat2gJ3dy+1ec6cicPt2zcxZ85SjbW5JPk0aY2qXh8pX7/MzgYApKckw8Lm1TqUnpKMyjU831iXXgV92Du7AgBcPWrh9pW/cCBqJfqPC1ebv6p33ucm3b9T5r8wc5ze3Ye67RV5sqiBgQHMzMxUFk0zNTVVfpF1c3ODp6cnHBwccPDgQWWe9PR0nDp1Cg0bNlRbh76+PurVq6dSRqFQ4ODBg8oyY8eOxd9//434+HjlAgCzZ8/GypXFj8JLQ1H6q86MGTPw888/IzY2Fr6+vkX6LIVCoTLdoTzQxDoF5N2oNmTIEMTExODQoUOoUqWKyvv16tVDhQoVVOq9du0a7t2798Z6yxJNjdV/ZWRk4NatW8rg5H0Yq3zF3f7eJjMzEzo6qrtnXV1dKBSKYtepTSVxfAAAfWMDWFayVC7WVawhtZbi7tm7yjxymRyPrzyGk7eT2jp0K+jCvoYD7p17VUZQCLh37i6cvF6Vkcvk2DxiM3T0dNE5ogv0DIp8Hq5M0dfXh5dXHcTFHVOmKRQKxMX9CR+fwvf9S5f+hgULZmH58ijUquVTaL7o6PXw9q4DDw9vTTa7xBhJTWDv7KpcnKpWh7m1La6cOaHMk5XxHP9eji9wH8DbCAqF8gu4OveuXwYAmNuU/VkAHKd396Fue2V6TymRSDB8+HBMmTIF1atXR5UqVTB+/Hg4OTmpPL2pVatW6Ny5M4YMGQIACAsLQ//+/eHr64sGDRpgzpw5kMlkCAkJAZB3xlTd2cTKlSsX+MJYHrytv/369UPFihURHp53NiAiIgITJkzAhg0b4OrqqpzXbmJiAhMTE8hkMkydOhUdO3aEo6MjkpOTsWDBAjx8+BDdunUrtX5qQnHXqdDQUGzYsAHbt29X3icA5N3MbGRkBHNzcwwaNAhhYWGwsrKCmZkZhg4dioYNG5a7pxzlK+5YjRo1Ch06dICLiwsePXqEiRMnQldXF7169QKA926s3nX7y87OxpUrV5R/P3z4EPHx8TAxMYGbmxsAoEOHDpg6dSoqV64MLy8vXLhwAb/++isGDhxYOp0soyQSCep288XJ1XGwdLaEuaMFTiz7EybWJnBrUl2Zb9OwjXBrWgN1g/O+8Pj29MWeqbth7+4ARw9HnNt0FjlZOfBuVwtAXlARPWITcuQv0W5CO2TL5MiW5Z1UMbIwho5u+bqBOyTkK4wZMxTe3j6oXbsuVq9ejKysTAQH522To0eHwt7eAaNGjQcALFkyD3PnRuDXXyNRsaIznjzJuyJnbCyFVPrqylBGxnPExu7A2LHanwZXUiQSCVr3HISdK36DvXMV2Do5IyZyFixs7FC3WYAy3y/f9ELd5oFo1X0AACB6QQRqNWwOawcnvMiU4eTe7bh2/iTC5q0FACQ9uIuTe7ehdqOWMDG3wP2bV7Fx9k+o8ZEfnKurf3pZWcZxKpoPcdsr04EFAHz33XeQyWT48ssvkZqaisaNGyM2NlblfoBbt24hOTlZ+bpHjx548uQJJkyYgISEBPj4+CA2NrbADZbvi7f19969eypnPxctWoTs7Gzloz7zTZw4EZMmTYKuri6uXr2K1atXIzk5GdbW1qhfvz7+/PNPeHmpvyRXnhRnnVq0aBEAoHnz5ip1rVy5EgMGDACQd8VLR0cHwcHBkMvlCAwMxMKFC7XeH20qzlg9ePAAvXr1wtOnT2Fra4vGjRvj5MmTsLV9dQPb+zRW77r9PXr0CB999GpKwcyZMzFz5kw0a9ZMOQ3zt99+w/jx4/HNN98gKSkJTk5OGDx4MCZMmFCifSsPGvRpgJwX2dg3Yx/kGS9QsVYlBM/qpnKFIfVhKrJee1SpeysPZKZm4cSy48hMkcHWzQ5dZ3WD1CpvKlTitUTlD+Yt66E6zeCLzYNh7mheAj3TnHbtOiMl5SnmzYvAkydJ8PDwxvLlUbD5/zPAjx8/gI7Oq99z+v33VcjJycbQoaqB7JAho/Htt98pX+/cGQNBENC+ffn8DZrCtO33FeQvMrF62jhkZqSjeh1fhM1dgwqvzWNPengPz1OfKV+npyRj2eQwpCUnwcjEFJXc3BE2by28/JoAAPQqVMCV0yew//cVkL/IgpW9I+q1aIsOA4eWeP80heP0dh/itlemf8eCiIi0r7z+jsX7oCR+x+J9URK/Y0EfFm3/jsX74r35HQsiIiIiIiofGFgQEREREZFoDCyIiIiIiEg0BhZERERERCQaAwsiIiIiIhKNgQUREREREYnGwIKIiIiIiERjYEFERERERKIxsCAiIiIiItEYWBARERERkWgMLIiIiIiISDQGFkREREREJBoDCyIiIiIiEo2BBRERERERicbAgoiIiIiIRGNgQUREREREojGwICIiIiIi0RhYEBERERGRaAwsiIiIiIhINAYWREREREQkGgMLIiIiIiISjYEFERERERGJxsCCiIiIiIhEY2BBRERERESiMbAgIiIiIiLRGFgQEREREZFoDCyIiIiIiEg0BhZERERERCQaAwsiIiIiIhKNgQUREREREYnGwIKIiIiIiETTK+0GEBHR+ynEdhhMTExLuxll2vHUzNJuAr2HrmQvKO0mlA+poaXdgnIhK6Po+ylesSAiIiIiItEYWBARERERkWgMLIiIiIiISDQGFkREREREJBoDCyIiIiIiEo2BBRERERERicbAgoiIiIiIRGNgQUREREREojGwICIiIiIi0RhYEBERERGRaAwsiIiIiIhINAYWREREREQkGgMLIiIiIiISjYEFERERERGJxsCCiIiIiIhEY2BBRERERESiMbAgIiIiIiLRGFgQEREREZFoDCyIiIiIiEg0BhZERERERCQaAwsiIiIiIhKNgQUREREREYnGwIKIiIiIiERjYEFERERERKIxsCAiIiIiItEYWBARERERkWgMLIiIiIiISDQGFkREREREJBoDCyIiIiIiEo2BBRERERERicbAgoiIiIiIRGNgQUREREREopX5wGLBggVwdXWFoaEh/Pz8cPr06ULzbt26Fb6+vrCwsIBUKoWPjw/Wrl1bIE9AQACsra0hkUgQHx+v5R6UDEEQMGHCBDg6OsLIyAj+/v64cePGG8ssWrQItWvXhpmZGczMzNCwYUPs2bNHJc+tW7fQuXNn2NrawszMDN27d0diYqI2u6J1xRmr8PBw1K9fH6amprCzs0NQUBCuXbumkufFixcIDQ2FtbU1TExMEBwcXK7H6l22PQDYvHkz3N3dYWhoiFq1amH37t0q7ycmJmLAgAFwcnKCsbEx2rRp89ZxLy+Ks069bvr06ZBIJBg+fLgy7c6dO5BIJGqXzZs3a6EX5c+6dcvRokVdeHtXQteugfjrr/OF5o2KWotevdrD19cNvr5u6N8/uED+GjVs1S7Lls3Xdle0ThAExCyehRFtfTG4SQ38Etobifduv7HM4ei1mNA7EN+08MI3LbwwdWAQ/v7fYZU8EV/1wMAGLirLmvDvtdkVreNYFY0gCDi+7E8s6rQAc1r+ik3DovDsfspby13Ych5LukZidstZWPfFWjy+8lj5XlZ6Fg7OPoDlvZZiTstfsbjLIhyccwDyDLk2u6J1H9I6VaYDi6ioKISFhWHixIk4f/486tSpg8DAQCQlJanNb2VlhR9++AFxcXH4+++/ERISgpCQEOzdu1eZRyaToXHjxoiIiCipbpSIGTNmYN68eYiMjMSpU6cglUoRGBiIFy9eFFqmUqVKmD59Os6dO4ezZ8+iZcuW6NSpEy5fvgwgb6wCAgIgkUhw6NAhnDhxAtnZ2ejQoQMUCkVJdU3jijNWR48eRWhoKE6ePIn9+/cjJycHAQEBkMlkyjwjRozAjh07sHnzZhw9ehSPHj1Cly5dSqJLGveu297//vc/9OrVC4MGDcKFCxcQFBSEoKAgXLp0CUDeTjUoKAj//vsvtm/fjgsXLsDFxQX+/v4qY1heFWedynfmzBksXrwYtWvXVkl3dnbG48ePVZbJkyfDxMQEbdu21VZXyo1du2IQHj4BQ4aMwrZtB+Hu7oVBg7rj6dMnavOfPn0C7dt3wZo1MYiK2gNHRycMHNgNCQmvvtScOHFJZQkPnwuJRIKAgPYl1S2t2bMmEgeiVqHf2Gn4ccV2GBgZY9a3fZEjL3wdtbR3RNfQMZi4eicmrNoBd99G+G3UF3h467pKvqZBvTB79xnl0m3oOG13R6s4VkVzev1pXIg+j9ajAtBnyWeoYFQB0WGb8VL+stAyVw/+gyPzD6NhyCfou7w/7NxsER22CbJneceBjOQMZCRnoHloCwxYG4K2P3yKOydvI3b6nkLrLA8+pHVKIgiCUJyC6enpMDc3R1paGszMzDTdLgCAn58f6tevj/nz884WKRQKODs7Y+jQoRg7dmyR6qhbty7atWuHn3/+WSX9zp07qFKlCi5cuAAfHx9NN71ECYIAJycnjBw5EqNGjQIApKWlwd7eHqtWrULPnj2LXJeVlRV++eUXDBo0CPv27UPbtm3x7Nkz5f84LS0NlpaW2LdvH/z9/bXSH23S1Fg9efIEdnZ2OHr0KJo2bYq0tDTY2tpiw4YN6Nq1KwDg6tWr8PDwQFxcHD7++GOt9Ukb3nXb69GjB2QyGXbu3KlM+/jjj+Hj44PIyEhcv34dNWvWxKVLl+Dl5aWs08HBAdOmTcPnn39eMh3TAjHrVEZGBurWrYuFCxdiypQp8PHxwZw5cwrN/9FHH6Fu3bpYvny5Rvug6f15fn3nz/8LExNTDbSwoK5dA1Grlg8mTsw7SaRQKNC0aR307fs5Bg8e9tbyubm58PV1w4QJ09G5cw+1eb7+uh9ksgysWbNVo21/3fHUTK3VnU8QBIR9Wh+Bfb5Am88GAwAyM9IxvI0vBk2YCb+AjkWua6h/bXQb+j2adspbryO+6gHnGp7oHTZRK20vae/LWF3JXqDV+gVBQGTQQvj2qI/6vRsAAOQZcizsOB9tv/8U7v4easut+2ItHDwc4B/WOq8ehYDFXRbho+C68Our/jh57dBV7P55F4btHwEdPc2eD/fUD9Vofeq8D+tUVsZzhLb0LtIxosxescjOzsa5c+dUvrzq6OjA398fcXFxby0vCAIOHjyIa9euoWnTptpsaqm7ffs2EhISVMbK3Nwcfn5+RRorIO8gu3HjRshkMjRs2BAAIJfLIZFIYGBgoMxnaGgIHR0dHD9+XLOdKCGaGCsg74sjkBeIAcC5c+eQk5OjUq+7uzsqV678TvWWBcXZ9uLi4goEmoGBgcr8cnneZWxDQ0OVOg0MDMrtupRPzDoVGhqKdu3aFSlIP3fuHOLj4zFo0CDRbS7vsrOzcfnyX2jUqJkyTUdHB40aNUV8/Nki1ZGVlYWXL1/CwsJS7fvJyUk4enQ/unXro5E2l6Ynj+4j7ekTeDZorEwzNjFDVS8f3LpY+PSx1ylyc3Fq3x+QZ2WhWq26Ku+djN2Gb1v7YHzP1oheEAH5iyyNtr8kcayKJu1RGmRPZXCp76JMMzAxgKOnIx5deqS2TG5OLhKvJ8DF11WZJtGRoLKvCx5dVl8GAOQyOfSl+hoPKkrKh7ZO6RU1o1wuV345APLOSGlTcnIycnNzYW9vr5Jub2+Pq1evFlouLS0NFStWhFwuh66uLhYuXIjWrVtrta2lLSEhAQDUjlX+e4W5ePEiGjZsiBcvXsDExAQxMTHw9PQEkHfGWSqVYsyYMZg2bRoEQcDYsWORm5uLx48fv7HeskrMWOVTKBQYPnw4PvnkE3h7eyvr1dfXh4WFRbHrLSuKs+0lJCS8cUzzg6xx48Zh8eLFkEqlmD17Nh48eFBu16V8xV2nNm7ciPPnz+PMmTNF+pzly5fDw8MDjRo1Kn5jtaSkjw/PnqUgNzcXNja2Kuk2Nnb499+bRapj5syfYGfngEaN1J94iomJglRqgoCAdqLbW9rSn+ZNYTSzslFJN7OyQVohU8fyPbh5FVMHdUZOthwGRlIMmbEYFavWUL7vF9gJNg4VYWFrj/s3/0H0/OlIuHsLQ2Ys0XxHSgDHqmhkKXlTl4wtpSrpxpZSyFIy1JbJSsuEkCtAamWski61kiLlrvp7MzJTMxG3Kg61O9TRQKtLx4e2ThU5/AsPD4e5ublycXZ21ma7is3U1BTx8fE4c+YMpk6dirCwMBw5cqS0m6VR69evh4mJiXLJyckpdl01a9ZEfHw8Tp06ha+//hr9+/fHlStXAAC2trbYvHkzduzYARMTE5ibmyM1NRV169aFjk75OHOgybHKFxoaikuXLmHjxo0aaOGHoUKFCti6dSuuX78OKysrGBsb4/Dhw2jbtm25WZfyaWKdun//PoYNG4b169erXMUpTFZWFjZs2FBmr1aUl+NDvsWL52LXrhgsWLAKBgbqxz86egM6dAgu9P2yLC42Bl8381AuuS8Ln/P+Ng4uVTFp3R78uGI7WgR/hmWTR+Lhv6/meDfv3BveDZuhkps7GrbpjM8n/orzR/Yi6cFdTXRF6zhWRXNl32XMbT1buShe5mr9M+UyObaO3gJrV2s0GvSJ1j9PUz70darIVyzGjRuHsLAw5ev09HStHjxsbGygq6tb4Kk6iYmJcHBwKLScjo4O3NzcAAA+Pj74559/EB4ejubNm2utrSWtY8eO8PPzU77OP1OYmJgIR0dHZXpiYuJb7x/R19dXjle9evVw5swZzJ07F4sXLwYABAQE4NatW0hOToaenh4sLCzg4OCAqlWrarhX2qHJsQKAIUOGYOfOnTh27BgqVaqkTHdwcEB2djZSU1NVrlq8bX0ti4qz7Tk4OLw1f7169RAfH4+0tDRkZ2fD1tYWfn5+8PX11XwntEgT69S5c+eQlJSEunVfXdLOzc3FsWPHMH/+fOUV13zR0dHIzMxEv379NNwbzSjp44OlpRV0dXWRnKx6ti85OQm2tnZvLLt8+QIsWTIPq1Ztgbu7l9o8Z87E4fbtm5gzZ6nG2lySfJq0RlWvj5SvX2ZnAwDSU5JhYfPqylp6SjIq1/B8Y116FfRh7+wKAHD1qIXbV/7CgaiV6D8uXG3+qt55n5t0/w7sKrmozVOWcKyKxq2xGxw9nZSvc7PzAovMZzKY2Jgo0zOfyWDnZl+gPAAYmRtDoiuBLEX1viJZigxSa9UrH9mZcmwZuRn6xvoImtYZunq6KC8+9HWqyKcKDQwMlI8lzV+0SV9fH/Xq1cPBgweVaQqFAgcPHlTeA1AUCoVC5RL9+8DU1BRubm7KxdPTEw4ODipjlZ6ejlOnTr3TWAGFj5eNjQ0sLCxw6NAhJCUloWPHot9sVJo0NVaCIGDIkCGIiYnBoUOHUKVKFZX369WrhwoVKqjUe+3aNdy7d++d/welrTjbXsOGDVXyA8D+/fvV5jc3N4etrS1u3LiBs2fPolOnTprtgJZpYp1q1aoVLl68iPj4eOXi6+uLPn36ID4+XiWoAPKmQXXs2BG2trZq6yttpXF88PKqg7i4Y8o0hUKBuLg/4eNTeKC6dOlvWLBgFpYvj0KtWj6F5ouOXg9v7zrw8PDWZLNLjJHUBPbOrsrFqWp1mFvb4sqZE8o8WRnP8e/l+ALztd9GUCiUX5TUuXc976mC5jZvDvDKCo5V0egbG8CykqVysa5iDam1FHfPvjozLpfJ8fjKYzh5O6mtQ7eCLuxrOODeuVdlBIWAe+fuwsnrVRm5TI7NIzZDR08XnSO6QM+gyOfAy4QPfZ0q0/+tsLAw9O/fH76+vmjQoAHmzJkDmUyGkJAQAEC/fv1QsWJFhIfnRW7h4eHw9fVFtWrVIJfLsXv3bqxduxaLFi1S1pmSkoJ79+7h0aO8G4Xyf4vAwcGh3J1Zzpf//PspU6agevXqqFKlCsaPHw8nJycEBQUp87Vq1QqdO3fGkCFDAOSdZWzbti0qV66M58+fY8OGDThy5IjK43lXrlwJDw8P2NraIi4uDsOGDcOIESNQs2bNku6mRhR3rEJDQ7FhwwZs374dpqamyrnz5ubmMDIygrm5OQYNGoSwsDBYWVnBzMwMQ4cORcOGDcvdE6GAd9/2hg0bhmbNmmHWrFlo164dNm7ciLNnz2LJklfzPDdv3gxbW1tUrlwZFy9exLBhwxAUFISAgIBS6aOmFGedMjU1Vd6fk08qlcLa2rpA+s2bN3Hs2LECvwvyoQsJ+QpjxgyFt7cPateui9WrFyMrKxPBwb0AAKNHh8Le3gGjRo0HACxZMg9z50bg118jUbGiM548ybvCZmwshVT66oxrRsZzxMbuwNixk0u+U1oikUjQuucg7FzxG+ydq8DWyRkxkbNgYWOHus1ebX+/fNMLdZsHolX3AQCA6AURqNWwOawdnPAiU4aTe7fj2vmTCJuX9/tQSQ/u4uTebajdqCVMzC1w/+ZVbJz9E2p85Afn6uqfClTWcayKRiKRoG43X5xcHQdLZ0uYO1rgxLI/YWJtArcm1ZX5Ng3bCLemNVA3OO8LtG9PX+yZuhv27g5w9HDEuU1nkZOVA+92tQDkBRXRIzYhR/4S7Sa0Q7ZMjmxZ3slOIwtj6OiWr6mzwIe3TpXpwKJHjx548uQJJkyYgISEBPj4+CA2NlZ5k+S9e/dU5mfLZDJ88803ePDgAYyMjODu7o5169ahR49XjxL8448/lF+OACgfBTlx4kRMmjSpZDqmBd999x1kMhm+/PJLpKamonHjxoiNjVWZv50/pSlfUlIS+vXrh8ePH8Pc3By1a9fG3r17VW52v3btGsaNG4eUlBS4urrihx9+wIgRI0q0b5pWnLHKD07/O6Vu5cqVGDBgAABg9uzZ0NHRQXBwMORyOQIDA7Fw4UKt90cb3nXba9SoETZs2IAff/wR33//PapXr45t27apfEl+/PgxwsLClFOG+vXrh/Hjx5d437ShOOtUUa1YsQKVKlUq9wGYprVr1xkpKU8xb14EnjxJgoeHN5Yvj4LN/5+pe/z4AXR0JMr8v/++Cjk52Rg6dKBKPUOGjMa3336nfL1zZwwEQUD79uXzN2gK07bfV5C/yMTqaeOQmZGO6nV8ETZ3DSq8dg9J0sN7eJ76TPk6PSUZyyaHIS05CUYmpqjk5o6weWvh5dcEAKBXoQKunD6B/b+vgPxFFqzsHVGvRVt0GDi0xPunSRyromnQpwFyXmRj34x9kGe8QMValRA8q5vKFYbUh6nIeu2Ryu6tPJCZmoUTy44jM0UGWzc7dJ3VDVKrvKlQidcSlT+Yt6yH6lTELzYPhrmjeQn0TPM+pHWqTP+OBRERaV95/B2L90VJ/I4FfXi0/TsW74uS+B2L98F78TsWRERERERUfjCwICIiIiIi0RhYEBERERGRaAwsiIiIiIhINAYWREREREQkGgMLIiIiIiISjYEFERERERGJxsCCiIiIiIhEY2BBRERERESiMbAgIiIiIiLRGFgQEREREZFoDCyIiIiIiEg0BhZERERERCQaAwsiIiIiIhKNgQUREREREYnGwIKIiIiIiERjYEFERERERKIxsCAiIiIiItEYWBARERERkWgMLIiIiIiISDQGFkREREREJBoDCyIiIiIiEo2BBRERERERicbAgoiIiIiIRGNgQUREREREojGwICIiIiIi0RhYEBERERGRaAwsiIiIiIhINAYWREREREQkGgMLIiIiIiISjYEFERERERGJplfaDSAiovfTyidzYZBpUNrNoPeEp35oaTeh3OBYFU1jC+PSbkK5kKGXW+S8vGJBRERERESiMbAgIiIiIiLRGFgQEREREZFoDCyIiIiIiEg0BhZERERERCQaAwsiIiIiIhKNgQUREREREYnGwIKIiIiIiERjYEFERERERKIxsCAiIiIiItEYWBARERERkWgMLIiIiIiISDQGFkREREREJBoDCyIiIiIiEo2BBRERERERicbAgoiIiIiIRGNgQUREREREojGwICIiIiIi0RhYEBERERGRaAwsiIiIiIhINAYWREREREQkGgMLIiIiIiISjYEFERERERGJxsCCiIiIiIhEY2BBRERERESiMbAgIiIiIiLRGFgQEREREZFoDCyIiIiIiEg0BhZERERERCQaAwsiIiIiIhKNgQUREREREYlW5gMLQRAwYcIEODo6wsjICP7+/rhx48Ybyxw7dgwdOnSAk5MTJBIJtm3bpjbfP//8g44dO8Lc3BxSqRT169fHvXv3tNAL7SvOOAHAggUL4OrqCkNDQ/j5+eH06dMq7yckJKBv375wcHCAVCpF3bp1sWXLFm11o0QUZ6wWLVqE2rVrw8zMDGZmZmjYsCH27NmjkufWrVvo3LkzbG1tYWZmhu7duyMxMVGbXdG64oxVeHg46tevD1NTU9jZ2SEoKAjXrl1TybNkyRI0b94cZmZmkEgkSE1N1WIvtO9t29Hrli5diiZNmsDS0hKWlpbw9/cvkF8ikahdfvnlF213pdwRBAHHl/2JRZ0WYE7LX7FpWBSe3U95a7kLW85jSddIzG45C+u+WIvHVx4r38tKz8LB2QewvNdSzGn5KxZ3WYSDcw5AniHXZle0iuNUdIIgIGbxLIxo64vBTWrgl9DeSLx3+41lDkevxYTegfimhRe+aeGFqQOD8Pf/DqvkifiqBwY2cFFZ1oR/r82uaBXHqWjWrVuOFi3qwtu7Erp2DcRff50vNG9U1Fr06tUevr5u8PV1Q//+wQXy16hhq3ZZtmy+trtSJGU+sJgxYwbmzZuHyMhInDp1ClKpFIGBgXjx4kWhZWQyGerUqYMFCxYUmufWrVto3Lgx3N3dceTIEfz9998YP348DA0NtdENrSvOOEVFRSEsLAwTJ07E+fPnUadOHQQGBiIpKUmZp1+/frh27Rr++OMPXLx4EV26dEH37t1x4cKFkuiWVhRnrCpVqoTp06fj3LlzOHv2LFq2bIlOnTrh8uXLAPLWuYCAAEgkEhw6dAgnTpxAdnY2OnToAIVCUVJd07jijNXRo0cRGhqKkydPYv/+/cjJyUFAQABkMpkyT2ZmJtq0aYPvvy+/B4t8RdmOXnfkyBH06tULhw8fRlxcHJydnREQEICHDx8q8zx+/FhlWbFiBSQSCYKDg0uqW+XG6fWncSH6PFqPCkCfJZ+hglEFRIdtxkv5y0LLXD34D47MP4yGIZ+g7/L+sHOzRXTYJsie5a2jGckZyEjOQPPQFhiwNgRtf/gUd07eRuz0PYXWWdZxnIpuz5pIHIhahX5jp+HHFdthYGSMWd/2RY688P2epb0juoaOwcTVOzFh1Q64+zbCb6O+wMNb11XyNQ3qhdm7zyiXbkPHabs7WsNxertdu2IQHj4BQ4aMwrZtB+Hu7oVBg7rj6dMnavOfPn0C7dt3wZo1MYiK2gNHRycMHNgNCQmvAvoTJy6pLOHhcyGRSBAQ0L6kuvVGEkEQhOIUTE9Ph7m5OdLS0mBmZqbpdgHIi4adnJwwcuRIjBo1CgCQlpYGe3t7rFq1Cj179nxrHRKJBDExMQgKClJJ79mzJypUqIC1a9dqo+klqrjj5Ofnh/r162P+/LwoV6FQwNnZGUOHDsXYsWMBACYmJli0aBH69u2rLGdtbY2IiAh8/vnnWu6Z5mlincpnZWWFX375BYMGDcK+ffvQtm1bPHv2TLk9pKWlwdLSEvv27YO/v79W+qNNmhqrJ0+ewM7ODkePHkXTpk1V3jty5AhatGiBZ8+ewcLCQtNdKBFF2Y7eJDc3F5aWlpg/fz769eunNk9QUBCeP3+OgwcParTt+TS9P8+vb+jeYTCQGmigheoJgoDIoIXw7VEf9Xs3AADIM+RY2HE+2n7/Kdz9PdSWW/fFWjh4OMA/rHVePQoBi7sswkfBdeHX92O1Za4duordP+/CsP0joKNX5s/JqXhfxslTP1Sj9akjCALCPq2PwD5foM1ngwEAmRnpGN7GF4MmzIRfQMci1zXUvza6Df0eTTvl7SsjvuoB5xqe6B02USttL0nvyzg1tjDWav1duwaiVi0fTJwYASDv+NC0aR307fs5Bg8e9tbyubm58PV1w4QJ09G5cw+1eb7+uh9ksgysWbNVo21/XUbGc9StW7VIx4gyvXe8ffs2EhISVL6UmZubw8/PD3FxccWuV6FQYNeuXahRowYCAwNhZ2cHPz+/QqdMlXXFGafs7GycO3dOpYyOjg78/f1VyjRq1AhRUVFISUmBQqHAxo0b8eLFCzRv3lxr/dEmTaxTubm52LhxI2QyGRo2bAgAkMvlkEgkMDB49SXK0NAQOjo6OH78uGY7UUI0tf2lpaUByAvE3jdF3Y7eJDMzEzk5OYWOT2JiInbt2oVBgwZppM3vk7RHaZA9lcGlvosyzcDEAI6ejnh06ZHaMrk5uUi8ngAXX1dlmkRHgsq+Lnh0WX0ZAJDL5NCX6pe7oALgOL2LJ4/uI+3pE3g2aKxMMzYxQ1UvH9y6WPgUltcpcnNxat8fkGdloVqtuirvnYzdhm9b+2B8z9aIXhAB+Yssjba/pHCc3i47OxuXL/+FRo2aKdN0dHTQqFFTxMefLVIdWVlZePnyJSwsLNW+n5ychKNH96Nbtz4aabMm6BU1o1wuh1z+at5kenq6Vhr0uoSEBACAvb29Srq9vb3yveJISkpCRkYGpk+fjilTpiAiIgKxsbHo0qULDh8+jGbNmr29kjKkOOOUnJyM3NxctWWuXr2qfL1p0yb06NED1tbW0NPTg7GxMWJiYuDm5qbhXpQMMevUxYsX0bBhQ7x48QImJiaIiYmBp6cnAODjjz+GVCrFmDFjMG3aNAiCgLFjxyI3NxePHz9+Y71llSa2P4VCgeHDh+OTTz6Bt7e3xttY2oq6Hb3JmDFj4OTkVOhVrdWrV8PU1BRdunQR3V5tKY3jAwDIUvKm5BhbSlXSjS2lkKVkqC2TlZYJIVeA1Er1TKXUSoqUu+rvOchMzUTcqjjU7lBHA60ueRynokt/mjeF0czKRiXdzMoGaYVMX8n34OZVTB3UGTnZchgYSTFkxmJUrFpD+b5fYCfYOFSEha097t/8B9HzpyPh7i0MmbFE8x3RMo7T2z17loLc3FzY2NiqpNvY2OHff28WqY6ZM3+CnZ0DGjVqqvb9mJgoSKUmCAhoJ7q9mlLkUwrh4eEwNzdXLs7OzhpvzPr162FiYqJccnJyNP4ZAJRz3jt16oQRI0bAx8cHY8eORfv27REZGamVz9SkkhonABg/fjxSU1Nx4MABnD17FmFhYejevTsuXryotc/UJE2OVc2aNREfH49Tp07h66+/Rv/+/XHlyhUAgK2tLTZv3owdO3bAxMQE5ubmSE1NRd26daGjUz7O3GljvQoNDcWlS5ewceNGDbTw/TN9+nRs3LgRMTExhd7ftWLFCvTp06dM3/9VEscHALiy7zLmtp6tXBQvc7XyOa+Ty+TYOnoLrF2t0WjQJ1r/PE3gOBVdXGwMvm7moVxyXxZ+z8nbOLhUxaR1e/Djiu1oEfwZlk0eiYf/vrp3oHnn3vBu2AyV3NzRsE1nfD7xV5w/shdJD+5qoitaxXEqeYsXz8WuXTFYsGAVDAzU7/+jozegQ4fgQt8vDUW+YjFu3DiEhYUpX6enp2v84NGxY0f4+fkpX+efAUtMTISjo6MyPTExET4+PsX+HBsbG+jp6SnPNufz8PAoF9NWNDFONjY20NXVLfDUosTERDg4OADIu8F9/vz5uHTpEry8vAAAderUwZ9//okFCxaUiyBMk+uUvr6+8kpNvXr1cObMGcydOxeLFy8GAAQEBODWrVtITk6Gnp4eLCws4ODggKpVq2q4V9qh6e1vyJAh2LlzJ44dO4ZKlSppvL1lQVG2o8LMnDkT06dPx4EDB1C7dm21ef78809cu3YNUVFRGmuzNpTE8QEA3Bq7wdHTSfk6NzvvC3PmMxlMbEyU6ZnPZLBzsy9QHgCMzI0h0ZVAlpKpki5LkUFqrXpGPztTji0jN0PfWB9B0zpDV09XU13RKo5T0fk0aY2qXh8pX7/MzgYApKckw8Lm1dikpySjcg3PAuVfp1dBH/bOrgAAV49auH3lLxyIWon+48LV5q/qnfe5SffvwK6Si9o8ZQXH6d1ZWlpBV1cXycmqV3CSk5Nga2v3xrLLly/AkiXzsGrVFri7e6nNc+ZMHG7fvok5c5ZqrM2aUORTqQYGBspHbeYvmmZqago3Nzfl4unpCQcHB5UbFtPT03Hq1Cnl3Pbi0NfXR/369Qs8AvP69etwcSn7K60mxklfXx/16tVTKaNQKHDw4EFlmczMvAPKf8+46+rqlpsnHWlznVIoFCrTP/LZ2NjAwsIChw4dQlJSEjp2LPpNbKVJU2MlCAKGDBmCmJgYHDp0CFWqVCmJ5peKomxH6syYMQM///wzYmNj4evrW2i+5cuXo169eqhTp2xPLSmJ4wMA6BsbwLKSpXKxrmINqbUUd8++OpMpl8nx+MpjOHk7qa1Dt4Iu7Gs44N65V2UEhYB75+7CyetVGblMjs0jNkNHTxedI7pAz6DI5+FKHcep6IykJrB3dlUuTlWrw9zaFlfOnFDmycp4jn8vxxe4D+BtBIVC+QVcnXvX854qaG7z5i+ZZQHH6d3p6+vDy6sO4uKOKdMUCgXi4v6Ej0/h+/2lS3/DggWzsHx5FGrV8ik0X3T0enh714GHR9maZlym9wASiQTDhw/HlClTUL16dVSpUgXjx4+Hk5OTylOeWrVqhc6dO2PIkCEAgIyMDNy8+Wr+2u3btxEfHw8rKytUrlwZADB69Gj06NEDTZs2RYsWLRAbG4sdO3bgyJEjJdlFjSjuOIWFhaF///7w9fVFgwYNMGfOHMhkMoSEhAAA3N3d4ebmhsGDB2PmzJmwtrbGtm3bsH//fuzcubM0uipaccdq3LhxaNu2LSpXroznz59jw4YNOHLkCPbu3asss3LlSnh4eMDW1hZxcXEYNmwYRowYgZo1a5Z0NzWiuGMVGhqKDRs2YPv27TA1NVXej2Fubg4jIyMAefdvJCQkKLfTixcvwtTUFJUrVy53N3m/bTvq168fKlasiPDwvLNxERERmDBhAjZs2ABXV1fl+ORPQcuXnp6OzZs3Y9asWSXfqXJCIpGgbjdfnFwdB0tnS5g7WuDEsj9hYm0CtybVlfk2DdsIt6Y1UDc47wuPb09f7Jm6G/buDnD0cMS5TWeRk5UD73a1AOR9WY4esQk58pdoN6EdsmVyZMvyTiIYWRhDR7d8TG/Mx3EqOolEgtY9B2Hnit9g71wFtk7OiImcBQsbO9RtFqDM98s3vVC3eSBadR8AAIheEIFaDZvD2sEJLzJlOLl3O66dP4mweXlPnkx6cBcn925D7UYtYWJugfs3r2Lj7J9Q4yM/OFdX/1SusozjVDQhIV9hzJih8Pb2Qe3adbF69WJkZWUiOLgXAGD06FDY2ztg1KjxAIAlS+Zh7twI/PprJCpWdMaTJ3lXw42NpZBKXx0fMjKeIzZ2B8aOnVzynXqLMh1YAMB3330HmUyGL7/8EqmpqWjcuDFiY2NV5hvnTz/Jd/bsWbRo0UL5Ov8Sff/+/bFq1SoAQOfOnREZGYnw8HB8++23qFmzJrZs2YLGjV894aA8Kc449ejRA0+ePMGECROQkJAAHx8fxMbGKm9ErVChAnbv3o2xY8eiQ4cOyMjIgJubG1avXo1PP/20xPuoKcUZq6SkJPTr1w+PHz+Gubk5ateujb1796J169bKPNeuXcO4ceOQkpICV1dX/PDDDxgxYkSJ9k3TijNWixYtAoACTw5buXIlBgwYAACIjIzE5Mmvdoj5j6F9PU958bbt6N69eypX/RYtWoTs7Gx07dpVpZ6JEydi0qRJytcbN26EIAjo1atXifSjvGrQpwFyXmRj34x9kGe8QMValRA8q5vKmfPUh6nISn01pce9lQcyU7NwYtlxZKbIYOtmh66zukFqlTfFJ/FaovKH4Jb1UJ1m8MXmwTB3NC+BnmkWx6no2vb7CvIXmVg9bRwyM9JRvY4vwuauQYXX5rEnPbyH56nPlK/TU5KxbHIY0pKTYGRiikpu7gibtxZefk0AAHoVKuDK6RPY//sKyF9kwcreEfVatEWHgUNLvH+awnF6u3btOiMl5SnmzYvAkydJ8PDwxvLlUbD5/6svjx8/gI6ORJn/999XIScnG0OHDlSpZ8iQ0fj22++Ur3fujIEgCGjfvuw91KNM/44FERFpX3n9HQv6sJTE71jQh0Xbv2PxvnhvfseCiIiIiIjKBwYWREREREQkGgMLIiIiIiISjYEFERERERGJxsCCiIiIiIhEY2BBRERERESiMbAgIiIiIiLRGFgQEREREZFoDCyIiIiIiEg0BhZERERERCQaAwsiIiIiIhKNgQUREREREYnGwIKIiIiIiERjYEFERERERKIxsCAiIiIiItEYWBARERERkWgMLIiIiIiISDQGFkREREREJBoDCyIiIiIiEo2BBRERERERicbAgoiIiIiIRGNgQUREREREojGwICIiIiIi0RhYEBERERGRaAwsiIiIiIhINAYWREREREQkGgMLIiIiIiISjYEFERERERGJxsCCiIiIiIhEY2BBRERERESiMbAgIiIiIiLR9Eq7AURERB8qT/3Q0m4CvYcaWxiXdhPKhSVJv5R2E8oFuUxe5Ly8YkFERERERKIxsCAiIiIiItEYWBARERERkWgMLIiIiIiISDQGFkREREREJBoDCyIiIiIiEo2BBRERERERicbAgoiIiIiIRGNgQUREREREojGwICIiIiIi0RhYEBERERGRaAwsiIiIiIhINAYWREREREQkGgMLIiIiIiISjYEFERERERGJxsCCiIiIiIhEY2BBRERERESiMbAgIiIiIiLRGFgQEREREZFoDCyIiIiIiEg0BhZERERERCQaAwsiIiIiIhKNgQUREREREYnGwIKIiIiIiERjYEFERERERKIxsCAiIiIiItEYWBARERERkWgMLIiIiIiISDQGFkREREREJBoDCyIiIiIiEo2BBRERERERicbAgoiIiIiIRCvzgcWCBQvg6uoKQ0ND+Pn54fTp02/Mv3nzZri7u8PQ0BC1atXC7t27C+T5559/0LFjR5ibm0MqlaJ+/fq4d++etrpQIgRBwIQJE+Do6AgjIyP4+/vjxo0bbyxz7NgxdOjQAU5OTpBIJNi2bVuBPImJiRgwYACcnJxgbGyMNm3avLXesq44Y/W66dOnQyKRYPjw4YXW37Zt20LHtLx4l21v6dKlaNKkCSwtLWFpaQl/f/8C+d/HdSmftra/rVu3IiAgANbW1pBIJIiPj9dOB8oxQRBwfNmfWNRpAea0/BWbhkXh2f2Ut5a7sOU8lnSNxOyWs7Dui7V4fOWx8r2s9CwcnH0Ay3stxZyWv2Jxl0U4OOcA5BlybXZFqwRBQMziWRjR1heDm9TAL6G9kXjv9hvLHI5eiwm9A/FNCy9808ILUwcG4e//HVbJE/FVDwxs4KKyrAn/Xptd0TqOVdGsW7ccLVrUhbd3JXTtGoi//jpfaN6oqLXo1as9fH3d4Ovrhv79gwvkr1HDVu2ybNl8bXdF6z6k/VSZDiyioqIQFhaGiRMn4vz586hTpw4CAwORlJSkNv///vc/9OrVC4MGDcKFCxcQFBSEoKAgXLp0SZnn1q1baNy4Mdzd3XHkyBH8/fffGD9+PAwNDUuqW1oxY8YMzJs3D5GRkTh16hSkUikCAwPx4sWLQsvIZDLUqVMHCxYsUPu+IAgICgrCv//+i+3bt+PChQtwcXGBv78/ZDKZtrqidcUZq3xnzpzB4sWLUbt27ULzzJkzBxKJRJNNLnHvuu0dOXIEvXr1wuHDhxEXFwdnZ2cEBATg4cOHAN7fdSmfNra//DyNGzdGRESENpr9Xji9/jQuRJ9H61EB6LPkM1QwqoDosM14KX9ZaJmrB//BkfmH0TDkE/Rd3h92braIDtsE2bO8dTEjOQMZyRloHtoCA9aGoO0Pn+LOyduInb6npLqlcXvWROJA1Cr0GzsNP67YDgMjY8z6ti9y5IWvo5b2jugaOgYTV+/EhFU74O7bCL+N+gIPb11Xydc0qBdm7z6jXLoNHaft7mgVx+rtdu2KQXj4BAwZMgrbth2Eu7sXBg3qjqdPn6jNf/r0CbRv3wVr1sQgKmoPHB2dMHBgNyQkvPqifOLEJZUlPHwuJBIJAgLal1S3tOZD2k9JBEEQilMwPT0d5ubmSEtLg5mZmabbBQDw8/ND/fr1MX9+XrSqUCjg7OyMoUOHYuzYsQXy9+jRAzKZDDt37lSmffzxx/Dx8UFkZCQAoGfPnqhQoQLWrl2rlTaXBkEQ4OTkhJEjR2LUqFEAgLS0NNjb22PVqlXo2bPnW+uQSCSIiYlBUFCQMu369euoWbMmLl26BC8vLwB5/wMHBwdMmzYNn3/+uVb6o01ixiojIwN169bFwoULMWXKFPj4+GDOnDkqeeLj49G+fXucPXsWjo6OBca0vHjXbe+/cnNzYWlpifnz56Nfv37v5bqUT1vb3+vu3LmDKlWq4MKFC/Dx8dFg6/Noen+eX9/QvcNgIDXQQAvVEwQBkUEL4dujPur3bgAAkGfIsbDjfLT9/lO4+3uoLbfui7Vw8HCAf1jrvHoUAhZ3WYSPguvCr+/HastcO3QVu3/ehWH7R0BHT3Pn5Dz1QzVWV2EEQUDYp/UR2OcLtPlsMAAgMyMdw9v4YtCEmfAL6Fjkuob610a3od+jaae89Triqx5wruGJ3mETtdL2kva+jFVjC2Ot1t+1ayBq1fLBxIl5Jz0UCgWaNq2Dvn0/x+DBw95aPjc3F76+bpgwYTo6d+6hNs/XX/eDTJaBNWu2arTtr1uS9IvW6s73Puyn5DI5fgucW6RjRJm9YpGdnY1z587B399fmaajowN/f3/ExcWpLRMXF6eSHwACAwOV+RUKBXbt2oUaNWogMDAQdnZ28PPzK9fTVQDg9u3bSEhIUOm7ubk5/Pz8Ch2ropDL8y6nvX41R0dHBwYGBjh+/HjxG1yKxIxVaGgo2rVrV2Ady5eZmYnevXtjwYIFcHBw0Gi7S1Jxtr3/yszMRE5ODqysrAC8n+tSPm1tf/R2aY/SIHsqg0t9F2WagYkBHD0d8ejSI7VlcnNykXg9AS6+rso0iY4ElX1d8Oiy+jJA3oFVX6qv0YN1SXny6D7Snj6BZ4PGyjRjEzNU9fLBrYuFT195nSI3F6f2/QF5Vhaq1aqr8t7J2G34trUPxvdsjegFEZC/yNJo+0sSx+rtsrOzcfnyX2jUqJkyTUdHB40aNUV8/Nki1ZGVlYWXL1/CwsJS7fvJyUk4enQ/unXro5E2l6YPbT+lV9SMcrlc+eUAyDsjpU3JycnIzc2Fvb29Srq9vT2uXr2qtkxCQoLa/AkJCQCApKQkZGRkYPr06ZgyZQoiIiIQGxuLLl264PDhw2jWrJm6asu8/P69qe/F4e7ujsqVK2PcuHFYvHgxpFIpZs+ejQcPHuDx48dvr6AMKu5Ybdy4EefPn8eZM2cKzTNixAg0atQInTp10kxjS0lxtr3/GjNmDJycnJRftt/HdSmftra/8qSkjw/5ZCl5UwKMLaUq6caWUshSMtSWyUrLhJArQGqlekZXaiVFyl31c54zUzMRtyoOtTvU0UCrS17607wpjGZWNirpZlY2SCtk6kq+BzevYuqgzsjJlsPASIohMxajYtUayvf9AjvBxqEiLGztcf/mP4iePx0Jd29hyIwlmu9ICeBYvd2zZynIzc2FjY2tSrqNjR3+/fdmkeqYOfMn2Nk5oFGjpmrfj4mJglRqgoCAdqLbW9o+tP1UkQOL8PBwTJ48WZtt0TqFQgEA6NSpE0aMGAEA8PHxwf/+9z9ERkaWm8Bi/fr1GDx4sPL1rl27tPI5FSpUwNatWzFo0CBYWVlBV1cX/v7+aNu2LYo5g67EaWKs7t+/j2HDhmH//v2F3ovzxx9/4NChQ7hw4UKx2/q+mD59OjZu3IgjR44ox+t9WJfyldT2V56U1PHhyr7L2P/LPuXrLjOCtf6ZcpkcW0dvgbWrNRoN+kTrn6cJcbExKjcFD5+9sth1ObhUxaR1e5CV8RxnD+3GsskjMSYySvmFuXnn3sq8ldzcYWFth19CeyPpwV3YVXIprNoyg2NV8hYvnotdu2Kwdu02GBioP6ZGR29Ahw7Bhb5fln3o+6kiBxbjxo1DWFiY8nV6ejqcnZ210igAsLGxga6uLhITE1XSExMTC51m4uDg8Mb8NjY20NPTg6enp0oeDw+PcjUdo2PHjvDz81O+zj9TmJiYCEdHR2V6YmKi6PnY9erVQ3x8PNLS0pCdnQ1bW1v4+fnB19dXVL0lRRNjde7cOSQlJaFu3VeXtHNzc3Hs2DHMnz8fcrkchw4dwq1bt2BhYaFSNjg4GE2aNMGRI0c01idtK862l2/mzJmYPn06Dhw4UOAG9/K+LuUrye2vvCip44NbYzc4ejopX+dm5wIAMp/JYGJjokzPfCaDnZt9gfIAYGRuDImuBLKUTJV0WYoMUmvVM4rZmXJsGbkZ+sb6CJrWGbp6uprqilb5NGmNql4fKV+/zM4GAKSnJMPC5tW4pKcko3INzwLlX6dXQR/2zq4AAFePWrh95S8ciFqJ/uPC1eav6p33uUn375SLL8scq3dnaZl3cig5WfUKTnJyEmxt7d5YdvnyBViyZB5WrdoCd3cvtXnOnInD7ds3MWfOUo21uSR96PupIk/CMjAwgJmZmcqiTfr6+qhXrx4OHjyoTFMoFDh48CAaNmyotkzDhg1V8gPA/v37lfn19fVRv359XLt2TSXP9evX4eJSfjZqU1NTuLm5KRdPT084ODio9D09PR2nTp0qdKzelbm5OWxtbXHjxg2cPXu23Ez30cRYtWrVChcvXkR8fLxy8fX1RZ8+fRAfHw9dXV2MHTsWf//9t0oeAJg9ezZWriz+GbDSUJxtD8h7MtLPP/+M2NjYNwYL5XVdylca219ZV1LHB31jA1hWslQu1lWsIbWW4u7Zu8o8cpkcj688hpO3k9o6dCvowr6GA+6de1VGUAi4d+4unLxelZHL5Ng8YjN09HTROaIL9AyKfB6u1BlJTWDv7KpcnKpWh7m1La6cOaHMk5XxHP9eji9wD8DbCAqF8su3OveuXwYAmNu8+QtmWcGxenf6+vrw8qqDuLhjyjSFQoG4uD/h41P4vn/p0t+wYMEsLF8ehVq1fArNFx29Ht7edeDh4a3JZpeYD30/VfoteIOwsDD0798fvr6+aNCgAebMmQOZTIaQkBAAQL9+/VCxYkWEh+edDRg2bBiaNWuGWbNmoV27dti4cSPOnj2LJUtezV8cPXo0evTogaZNm6JFixaIjY3Fjh07ytUZ5f/K/02FKVOmoHr16qhSpQrGjx8PJycnlafMtGrVCp07d8aQIUMA5D3l6ObNV/Mhb9++jfj4eFhZWaFy5coA8n4XxNbWFpUrV8bFixcxbNgwBAUFISAgoET7qCnFGStTU1N4e6vu4KRSKaytrZXpDg4Oas/mV65cGVWqVNFqn7ThXbe9iIgITJgwARs2bICrq6vy3gITExOYmOSdoXnf1qV82tz+UlJScO/ePTx6lHezXv5JkcLWtw+NRCJB3W6+OLk6DpbOljB3tMCJZX/CxNoEbk2qK/NtGrYRbk1roG5w3hdD356+2DN1N+zdHeDo4Yhzm84iJysH3u1qAcg7WEeP2IQc+Uu0m9AO2TI5smV5V6aMLIyho1u+buCWSCRo3XMQdq74DfbOVWDr5IyYyFmwsLFD3Wavtr9fvumFus0D0ar7AABA9III1GrYHNYOTniRKcPJvdtx7fxJhM3Le6pi0oO7OLl3G2o3agkTcwvcv3kVG2f/hBof+cG5uvon3ZR1HKuiCQn5CmPGDIW3tw9q166L1asXIysrE8HBvQAAo0eHwt7eAaNGjQcALFkyD3PnRuDXXyNRsaIznjzJuyJubCyFVPrqLH5GxnPExu7A2LHle+r96z60/VSZDix69OiBJ0+eYMKECUhISICPjw9iY2OVN0neu3cPOjqvBq5Ro0bYsGEDfvzxR3z//feoXr06tm3bpvKlsHPnzoiMjER4eDi+/fZb1KxZE1u2bEHjxo0LfH558t1330Emk+HLL79EamoqGjdujNjYWJV7Am7duoXk5GTl67Nnz6JFixbK1/lTGfr3749Vq1YBAB4/foywsDDlNI9+/fph/PjxJdMpLSnOWH1o3nXbW7RoEbKzs9G1a1eVeiZOnIhJkyYBeD/XpXza2v7++OMPZTAHQPno2tfH9UPXoE8D5LzIxr4Z+yDPeIGKtSoheFY3lTN3qQ9TkZX6akqBeysPZKZm4cSy48hMkcHWzQ5dZ3WD1CpvikHitUTlD1Et66E6HeOLzYNh7mheAj3TrLb9voL8RSZWTxuHzIx0VK/ji7C5a1DhtTnsSQ/v4XnqM+Xr9JRkLJschrTkJBiZmKKSmzvC5q2Fl18TAIBehQq4cvoE9v++AvIXWbCyd0S9Fm3RYeDQEu+fJnGs3q5du85ISXmKefMi8ORJEjw8vLF8eRRs/v/qy+PHD6Cj8+r3nH7/fRVycrIxdOhAlXqGDBmNb7/9Tvl6584YCIKA9u27lExHSsiHtJ8q079jQURE2ldef8fifVASv2NBHx5t/47F+6IkfsfiffBe/I4FERERERGVHwwsiIiIiIhINAYWREREREQkGgMLIiIiIiISjYEFERERERGJxsCCiIiIiIhEY2BBRERERESiMbAgIiIiIiLRGFgQEREREZFoDCyIiIiIiEg0BhZERERERCQaAwsiIiIiIhKNgQUREREREYnGwIKIiIiIiERjYEFERERERKIxsCAiIiIiItEYWBARERERkWgMLIiIiIiISDQGFkREREREJBoDCyIiIiIiEo2BBRERERERicbAgoiIiIiIRGNgQUREREREojGwICIiIiIi0RhYEBERERGRaAwsiIiIiIhINAYWREREREQkGgMLIiIiIiISjYEFERERERGJxsCCiIiIiIhE0ytuQUEQAADp6ekaawwREZW8/P14eno6TE1NIZFIRNWXf3zIlslFt+19l5X9vLSbQO+hDL3c0m5CuSDnPqpI8vfl+fv2N5EIRcmlxoMHD+Ds7FycokREVEalpaXBzMxMVB08PhARvX/u37+PSpUqvTFPsQMLhUKBR48eaeTslqakp6fD2dkZ9+/fF31gfN9xrIqG41R0HKuiKYvjJAgCnj9/DlNTU5iZmYnep5fF4wNQNse+LOI4FR3Hqmg4TkVXFscq/xjh5OQEHZ0330VR7KlQOjo6b41aSouZmVmZ+WeUdRyrouE4FR3HqmjK2jiZm5trrK6yfHwAyt7Yl1Ucp6LjWBUNx6noytpYFfUYwZu3iYiIiIhINAYWREREREQk2nsVWBgYGGDixIkwMDAo7aaUeRyrouE4FR3Hqmg4TqWHY180HKei41gVDcep6Mr7WBX75m0iIiIiIqJ879UVCyIiIiIiKh0MLIiIiIiISDQGFkREREREJBoDCyIiIiIiEo2BBRERERERicbAgoiIiIiIRGNgQUREREREojGwICIiIiIi0f4P8M4RDyI9+BIAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 pts)"
      ],
      "metadata": {
        "id": "O4FIfKk8JQX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "마지막 코드 셀은 2비트 k-means quantization을 수행하고 quantization 전후의 텐서를 플롯합니다. 각 클러스터는 고유한 색상으로 렌더링되며, quantized 텐서들이 4($2^2$)가지 고유한 색상으로 표시됩니다.\n",
        "\n",
        "이러한 현상을 관찰한 것을 바탕으로 질문들에 답하세요."
      ],
      "metadata": {
        "id": "o5mT6vV_JS5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2.1 (5 pts)\n",
        "\n",
        "4비트로 k-means quantization이 수행되면, quantized 텐서에는 몇 개의 고유한 색상이 렌더링될까요?"
      ],
      "metadata": {
        "id": "DwERfTxNKFqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Answer:**\n",
        "\n",
        "4비트 k-means quantization이 수행되면, quantized 텐서에 $(2^4 = 16)$개의 고유한 색상이 렌더링됩니다. 이는 4비트로 0000부터 1111까지의 16가지 다른 상태나 조합을 나타낼 수 있으며, 이는 텐서 값이 그룹화될 수 있는 16개의 고유한 클러스터에 해당합니다."
      ],
      "metadata": {
        "id": "0PGiy4rGKPbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2.2 (5 pts)\n",
        "\n",
        "*n*-비트 k-means quantization이 수행되면, quantized 텐서에 몇 개의 고유한 색상이 렌더링될까요?"
      ],
      "metadata": {
        "id": "Dt1ZGParKRWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Answer:**\n",
        "\n",
        "*n*-비트 k-means quantization이 수행되면, quantized 텐서에는 $(2^n)$개의 고유한 색상이 렌더링 됩니다. 이는 *n*비트를 사용하여 $(2^n)$개의 다른 상태나 조합을 나타낼 수 있으며, 이는 텐서 값이 그룹화될 수 있는 $(2^n)$개의 고유한 클러스터에 해당합니다."
      ],
      "metadata": {
        "id": "gK8v80tnKZHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Means Quantization on Whole Model\n",
        "\n",
        "lab 1에서 했던 것과 유사하게, 이제 전체 모델을 quantizing하기 위해 k-means quantization 함수를 클래스로 래핑합니다. `KMeansQuantizer` 클래스에서는 모델 가중치가 변경될 때마다 codebooks(i.e., `centroids`와 `labels`)을 적용하거나 업데이트할 수 있도록 codebooks의 변화를 기록해야 합니다."
      ],
      "metadata": {
        "id": "Aa7KVrLrSlNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import parameter\n",
        "class KMeansQuantizer:\n",
        "    def __init__(self, model : nn.Module, bitwidth=4):\n",
        "        self.codebook = KMeansQuantizer.quantize(model, bitwidth)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def apply(self, model, update_centroids):\n",
        "        for name, param in model.named_parameters():\n",
        "            if name in self.codebook:\n",
        "                if update_centroids:\n",
        "                    update_codebook(param, codebook=self.codebook[name])\n",
        "                self.codebook[name] = k_means_quantize(\n",
        "                    param, codebook=self.codebook[name])\n",
        "\n",
        "    @staticmethod\n",
        "    @torch.no_grad()\n",
        "    def quantize(model: nn.Module, bitwidth=4):\n",
        "        codebook = dict()\n",
        "        if isinstance(bitwidth, dict):\n",
        "            for name, param in model.named_parameters():\n",
        "                if name in bitwidth:\n",
        "                    codebook[name] = k_means_quantize(param, bitwidth=bitwidth[name])\n",
        "        else:\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.dim() > 1:\n",
        "                    codebook[name] = k_means_quantize(param, bitwidth=bitwidth)\n",
        "        return codebook"
      ],
      "metadata": {
        "id": "eaMAP465Te-p"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 K-Means Quantization을 사용하여 모델을 8비트, 4비트, 2비트로 quantize해봅시다. *모델 크기를 계산할 때 codebooks의 저장 공간은 무시한다는 점을 유의하세요.*"
      ],
      "metadata": {
        "id": "od8iy9KjVhlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Note that the storage for codebooks is ignored when calculating the model size.')\n",
        "quantizers = dict()\n",
        "for bitwidth in [8, 4, 2]:\n",
        "    recover_model()\n",
        "    print(f'k-means quantizing model into {bitwidth} bits')\n",
        "    quantizer = KMeansQuantizer(model, bitwidth)\n",
        "    quantized_model_size = get_model_size(model, bitwidth)\n",
        "    print(f\"    {bitwidth}-bit k-means quantized model has size={quantized_model_size/MiB:.2f} MiB\")\n",
        "    quantized_model_accuracy = evaluate(model, dataloader['test'])\n",
        "    print(f\"    {bitwidth}-bit k-means quantized model has accuracy={quantized_model_accuracy:.2f}%\")\n",
        "    quantizers[bitwidth] = quantizer"
      ],
      "metadata": {
        "id": "CCI9sCUmVre-",
        "outputId": "ab3b304e-8ffc-412f-8c2b-44da14079a1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "df8fb43d752645a1bdb334933197c31b",
            "6b47d0da90e244359fdf53629485f8fd",
            "20bbbcbc5a8d4045849b13764281b462",
            "15f415670c72409f88342cfe98c5d116",
            "3d22bbfc716c4a39a9b51bc785987de9",
            "27ffbdafd1754e318f65a84975ee1f67",
            "a1ee49dab3184480834ec8a6c5b71745",
            "0e25b4ed545b44b2a2ce03f1f2261230",
            "f78c659010df4f45b64592380cf0ef22",
            "ab8db668bb4240b8957ad20be7f52c2a",
            "60afd4f777ba444988da504f0fe409c7",
            "0e9f43aeb2d64d599f237baece19a247",
            "04f16e4c89cc4b3b867ec12749abf44e",
            "d822ecda1d694ad78ed28c7d687b58a1",
            "9ac77c3f1b584160aa5cb6fe609ad948",
            "822630d35cda48659af632c457970fcf",
            "aceb8964b9b649fa8a1e1edff968a1aa",
            "a3e69fe342974aa4a4795a949c59fa61",
            "cfa39903549d432f969a4d6935c15cb1",
            "e2ec8b4eef1c4f288b9cfadadb090e1d",
            "0b131260245e4fed9d390ec74a542d19",
            "75353c9908bb4e2a93de2648990d0628",
            "75740d5554bc46f9951d177a66f28af8",
            "3c325fd3fd6446c09226d8d12b337904",
            "0be881a2000b4af98253f273ff138e9b",
            "4b01f22136404172be0ce2057f436823",
            "559b2ce0b1dd4fa5b7ed4e26db2178f1",
            "9b77dc98c8674896a5edc30aa8b649d1",
            "5bcc87a3557340e9b3b1c3d3ad2741b6",
            "907cd9102355486c9e12f5d5cd8019eb",
            "8d21baa3a17841b8ae2498179f9366d8",
            "ad42172bf8324cf2aabe99bbfab75fdf",
            "156a6c48869e48d58263a0c9856a70c9"
          ]
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note that the storage for codebooks is ignored when calculating the model size.\n",
            "k-means quantizing model into 8 bits\n",
            "    8-bit k-means quantized model has size=8.80 MiB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "eval:   0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df8fb43d752645a1bdb334933197c31b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    8-bit k-means quantized model has accuracy=92.76%\n",
            "k-means quantizing model into 4 bits\n",
            "    4-bit k-means quantized model has size=4.40 MiB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "eval:   0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e9f43aeb2d64d599f237baece19a247"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    4-bit k-means quantized model has accuracy=79.07%\n",
            "k-means quantizing model into 2 bits\n",
            "    2-bit k-means quantized model has size=2.20 MiB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "eval:   0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75740d5554bc46f9951d177a66f28af8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    2-bit k-means quantized model has accuracy=10.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trained K-Means Quantization\n",
        "\n",
        "마지막 셀의 결과에서 볼 수 있듯이, 모델을 적은 비트로 quantize할 때 정확도가 크게 떨어집니다. 따라서, 정확도를 회복하기 위해 **quantization-aware training**을 해야 합니다.\n",
        "\n",
        "k-means quantization-aware 훈련 동안, centroids도 업데이트됩니다. 이는 [Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization And Huffman Coding](https://arxiv.org/pdf/1510.00149.pdf)에서 제안되었습니다.\n",
        "\n",
        "centroids에 대한 그래디언트는 다음과 같이 계산됩니다,\n",
        "\n",
        "> $\\frac{\\partial \\mathcal{L} }{\\partial C_k} = \\sum_{j} \\frac{\\partial \\mathcal{L} }{\\partial W_{j}} \\frac{\\partial W_{j} }{\\partial C_k} = \\sum_{j} \\frac{\\partial \\mathcal{L} }{\\partial W_{j}} \\mathbf{1}(I_{j}=k)$\n",
        "\n",
        "여기서 $\\mathcal{L}$은 손실, $C_k$는 *k*-번째 centroid, $I_{j}$는 가중치 $W_{j}$의 라벨입니다.\n",
        "\n",
        "$\\mathbf{1}()$은 지시 함수이며, $\\mathbf{1}(I_{j}=k)$는 $1\\;\\mathrm{if}\\;I_{j}=k\\;\\mathrm{else}\\;0$, *즉*, $I_{j}==k$를 의미합니다.\n",
        "\n",
        "lab에서는 **간단히** 최신 가중치에 따라 centroids를 직접 업데이트합니다:\n",
        "\n",
        "> $C_k = \\frac{\\sum_{j}W_{j}\\mathbf{1}(I_{j}=k)}{\\sum_{j}\\mathbf{1}(I_{j}=k)}$"
      ],
      "metadata": {
        "id": "4jj3FnIIWVq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3 (10 pts)\n",
        "\n",
        "아래의 codebook update function을 완성하세요.\n",
        "\n",
        "**Hint**:\n",
        "\n",
        "위의 centroids를 업데이트하는 방정식은 실제로 동일한 클러스터에 있는 가중치의 `평균(mean)`을 업데이트된 centroid 값으로 사용하고 있습니다."
      ],
      "metadata": {
        "id": "Q_hfG16fLVda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_codebook(fp32_tensor: torch.Tensor, codebook: Codebook):\n",
        "    \"\"\"\n",
        "    update the centroids in the codebook using updated fp32_tensor\n",
        "    :param fp32_tensor: [torch.(cuda.)Tensor]\n",
        "    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)\n",
        "    \"\"\"\n",
        "    n_clusters = codebook.centroids.numel()\n",
        "    fp32_tensor = fp32_tensor.view(-1)\n",
        "    for k in range(n_clusters):\n",
        "    ############### YOUR CODE STARTS HERE ###############\n",
        "        # hint: one line of code\n",
        "        codebook.centroids[k] = 0\n",
        "    ############### YOUR CODE ENDS HERE #################"
      ],
      "metadata": {
        "id": "t8_FfiUf_1ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's run the following code cell to finetune the k-means quantized model to recover the accuracy. We will stop finetuning if accuracy drop is less than 0.5."
      ],
      "metadata": {
        "id": "Ho4RDW0ccw4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_drop_threshold = 0.5\n",
        "quantizers_before_finetune = copy.deepcopy(quantizers)\n",
        "quantizers_after_finetune = quantizers\n",
        "\n",
        "for bitwidth in [8, 4, 2]:\n",
        "    recover_model()\n",
        "    quantizer = quantizers[bitwidth]\n",
        "    print(f'k-means quantizing model into {bitwidth} bits')\n",
        "    quantizer.apply(model, update_centroids=False)\n",
        "    quantized_model_size = get_model_size(model, bitwidth)\n",
        "    print(f\"    {bitwidth}-bit k-means quantized model has size={quantized_model_size/MiB:.2f} MiB\")\n",
        "    quantized_model_accuracy = evaluate(model, dataloader['test'])\n",
        "    print(f\"    {bitwidth}-bit k-means quantized model has accuracy={quantized_model_accuracy:.2f}% before quantization-aware training \")\n",
        "    accuracy_drop = fp32_model_accuracy - quantized_model_accuracy\n",
        "    if accuracy_drop > accuracy_drop_threshold:\n",
        "        print(f\"        Quantization-aware training due to accuracy drop={accuracy_drop:.2f}% is larger than threshold={accuracy_drop_threshold:.2f}%\")\n",
        "        num_finetune_epochs = 5\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        best_accuracy = 0\n",
        "        epoch = num_finetune_epochs\n",
        "        while accuracy_drop > accuracy_drop_threshold and epoch > 0:\n",
        "            train(model, dataloader['train'], criterion, optimizer, scheduler,\n",
        "                  callbacks=[lambda: quantizer.apply(model, update_centroids=True)])\n",
        "            model_accuracy = evaluate(model, dataloader['test'])\n",
        "            is_best = model_accuracy > best_accuracy\n",
        "            best_accuracy = max(model_accuracy, best_accuracy)\n",
        "            print(f'        Epoch {num_finetune_epochs-epoch} Accuracy {model_accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')\n",
        "            accuracy_drop = fp32_model_accuracy - best_accuracy\n",
        "            epoch -= 1\n",
        "    else:\n",
        "        print(f\"        No need for quantization-aware training since accuracy drop={accuracy_drop:.2f}% is smaller than threshold={accuracy_drop_threshold:.2f}%\")"
      ],
      "metadata": {
        "id": "DR1Fu6Arfgnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Quantization"
      ],
      "metadata": {
        "id": "USisw8QcnPNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will implement and perform linear quantization.\n",
        "\n",
        "Linear quantization directly rounds the floating-point value into the nearest quantized integer after range truncation and scaling.\n",
        "\n",
        "[Linear quantization](https://arxiv.org/pdf/1712.05877.pdf) can be represented as\n",
        "\n",
        "$r = S(q-Z)$\n",
        "\n",
        "where $r$ is a floating point real number, $q$ is a *n*-bit integer, $Z$ is a *n*-bit integer, and $S$ is a floating point real number. $Z$ is quantization zero point and $S$ is quantization scaling factor. Both constant $Z$ and $S$ are quantization parameters."
      ],
      "metadata": {
        "id": "sEzwgl9mZ6F2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *n*-bit Integer\n",
        "\n",
        "A *n*-bit signed integer is usually represented in [two's complement](https://en.wikipedia.org/wiki/Two%27s_complement) notation.\n",
        "\n",
        "A *n*-bit signed integer can enode integers in the range $[-2^{n-1}, 2^{n-1}-1]$. For example, a 8-bit integer falls in the range [-128, 127]."
      ],
      "metadata": {
        "id": "c6vO1_VBdTzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_quantized_range(bitwidth):\n",
        "    quantized_max = (1 << (bitwidth - 1)) - 1\n",
        "    quantized_min = -(1 << (bitwidth - 1))\n",
        "    return quantized_min, quantized_max"
      ],
      "metadata": {
        "id": "uz51HcD8dTIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 4** (15 pts)\n",
        "\n",
        "Please complete the following linear quantization function.\n",
        "\n",
        "**Hint**:\n",
        "*   From $r=S(q-Z)$, we have $q = r/S + Z$.\n",
        "*   Both $r$ and $S$ are floating numbers, and thus we cannot directly add integer $Z$ to $r/S$. Therefore $q = \\mathrm{int}(\\mathrm{round}(r/S)) + Z$.\n",
        "*   To convert [`torch.FloatTensor`](https://pytorch.org/docs/stable/tensors.html) to [`torch.IntTensor`](https://pytorch.org/docs/stable/tensors.html), we could use [`torch.round()`](https://pytorch.org/docs/stable/generated/torch.round.html#torch.round), [`torch.Tensor.round()`](https://pytorch.org/docs/stable/generated/torch.Tensor.round.html#torch.Tensor.round), [`torch.Tensor.round_()`](https://pytorch.org/docs/stable/generated/torch.Tensor.round_) to first convert all values to floating integer, and then use [`torch.Tensor.to(torch.int8)`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to) to convert the data type from [`torch.float`](https://pytorch.org/docs/stable/tensors.html) to [`torch.int8`](https://pytorch.org/docs/stable/tensors.html).\n",
        "\n"
      ],
      "metadata": {
        "id": "FSRwyEWrnA7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype=torch.int8) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    linear quantization for single fp_tensor\n",
        "      from\n",
        "        fp_tensor = (quantized_tensor - zero_point) * scale\n",
        "      we have,\n",
        "        quantized_tensor = int(round(fp_tensor / scale)) + zero_point\n",
        "    :param tensor: [torch.(cuda.)FloatTensor] floating tensor to be quantized\n",
        "    :param bitwidth: [int] quantization bit width\n",
        "    :param scale: [torch.(cuda.)FloatTensor] scaling factor\n",
        "    :param zero_point: [torch.(cuda.)IntTensor] the desired centroid of tensor values\n",
        "    :return:\n",
        "        [torch.(cuda.)FloatTensor] quantized tensor whose values are integers\n",
        "    \"\"\"\n",
        "    assert(fp_tensor.dtype == torch.float)\n",
        "    assert(isinstance(scale, float) or\n",
        "           (scale.dtype == torch.float and scale.dim() == fp_tensor.dim()))\n",
        "    assert(isinstance(zero_point, int) or\n",
        "           (zero_point.dtype == dtype and zero_point.dim() == fp_tensor.dim()))\n",
        "\n",
        "    ############### YOUR CODE STARTS HERE ###############\n",
        "    # Step 1: scale the fp_tensor\n",
        "    scaled_tensor = 0\n",
        "    # Step 2: round the floating value to integer value\n",
        "    rounded_tensor = 0\n",
        "    ############### YOUR CODE ENDS HERE #################\n",
        "\n",
        "    rounded_tensor = rounded_tensor.to(dtype)\n",
        "\n",
        "    ############### YOUR CODE STARTS HERE ###############\n",
        "    # Step 3: shift the rounded_tensor to make zero_point 0\n",
        "    shifted_tensor = 0\n",
        "    ############### YOUR CODE ENDS HERE #################\n",
        "\n",
        "    # Step 4: clamp the shifted_tensor to lie in bitwidth-bit range\n",
        "    quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
        "    quantized_tensor = shifted_tensor.clamp_(quantized_min, quantized_max)\n",
        "    return quantized_tensor"
      ],
      "metadata": {
        "id": "1vg4SfIJYFUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's verify the functionality of defined linear quantization by applying the function above on a dummy tensor."
      ],
      "metadata": {
        "id": "Qn-OU5z9IAtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_linear_quantize()"
      ],
      "metadata": {
        "id": "10sN7WRNAWCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5 (10 pts)\n",
        "\n",
        "Now we have to determine the scaling factor $S$ and zero point $Z$ for linear quantization.\n",
        "\n",
        "Recall that [linear quantization](https://arxiv.org/pdf/1712.05877.pdf) can be represented as\n",
        "\n",
        "$r = S(q-Z)$"
      ],
      "metadata": {
        "id": "y9WBvG1jnhEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scale\n",
        "\n",
        "Linear quantization projects the floating point range [*fp_min*, *fp_max*] to the quantized range [*quantized_min*, *quantized_max*]. That is to say,\n",
        "\n",
        "> $r_{\\mathrm{max}} = S(q_{\\mathrm{max}}-Z)$\n",
        ">\n",
        "> $r_{\\mathrm{min}} = S(q_{\\mathrm{min}}-Z)$\n",
        "\n",
        "Substracting these two equations, we have,\n"
      ],
      "metadata": {
        "id": "lLgyax_fgrS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 5.1 (1 pts)\n",
        "\n",
        "Please select the correct answer and delete the wrong answers in the next text cell."
      ],
      "metadata": {
        "id": "PkSalSPzSwN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> $S=r_{\\mathrm{max}} / q_{\\mathrm{max}}$\n",
        "\n",
        "> $S=(r_{\\mathrm{max}} + r_{\\mathrm{min}}) / (q_{\\mathrm{max}} + q_{\\mathrm{min}})$\n",
        "\n",
        "> $S=(r_{\\mathrm{max}} - r_{\\mathrm{min}}) / (q_{\\mathrm{max}} - q_{\\mathrm{min}})$\n",
        "\n",
        "> $S=r_{\\mathrm{max}} / q_{\\mathrm{max}} - r_{\\mathrm{min}} / q_{\\mathrm{min}}$"
      ],
      "metadata": {
        "id": "dctWTxHaTnUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "There are different approaches to determine the $r_{\\mathrm{min}}$ and  $r_{\\mathrm{max}}$ of a floating point tensor `fp_tensor`.\n",
        "\n",
        "*   The most common method is directly using the minimum and maximum value of `fp_tensor`.\n",
        "*   Another widely used method is minimizing Kullback-Leibler-J divergence to determine the *fp_max*."
      ],
      "metadata": {
        "id": "WWfff4eISuES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### zero point\n",
        "\n",
        "Once we determine the scaling factor $S$, we can directly use the relationship between $r_{\\mathrm{min}}$ and $q_{\\mathrm{min}}$ to calculate the zero point $Z$."
      ],
      "metadata": {
        "id": "SSb42B8Gjlpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 5.2 (1 pts)\n",
        "\n",
        "Please select the correct answer and delete the wrong answers in the next text cell."
      ],
      "metadata": {
        "id": "u13Wmw_8TdDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> $Z = \\mathrm{int}(\\mathrm{round}(r_{\\mathrm{min}} / S - q_{\\mathrm{min}})$\n",
        "\n",
        "> $Z = \\mathrm{int}(\\mathrm{round}(q_{\\mathrm{min}} - r_{\\mathrm{min}} / S))$\n",
        "\n",
        "> $Z = q_{\\mathrm{min}} - r_{\\mathrm{min}} / S$\n",
        "\n",
        "> $Z = r_{\\mathrm{min}} / S - q_{\\mathrm{min}}$"
      ],
      "metadata": {
        "id": "uD3Q_FfSTwIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 5.3 (8 pts)\n",
        "\n",
        "Please complete the following function for calculating the scale $S$ and zero point $Z$ from floating point tensor $r$.\n"
      ],
      "metadata": {
        "id": "tk4aPTKdAikt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_quantization_scale_and_zero_point(fp_tensor, bitwidth):\n",
        "    \"\"\"\n",
        "    get quantization scale for single tensor\n",
        "    :param fp_tensor: [torch.(cuda.)Tensor] floating tensor to be quantized\n",
        "    :param bitwidth: [int] quantization bit width\n",
        "    :return:\n",
        "        [float] scale\n",
        "        [int] zero_point\n",
        "    \"\"\"\n",
        "    quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
        "    fp_max = fp_tensor.max().item()\n",
        "    fp_min = fp_tensor.min().item()\n",
        "\n",
        "    ############### YOUR CODE STARTS HERE ###############\n",
        "    # hint: one line of code for calculating scale\n",
        "    scale = 0\n",
        "    # hint: one line of code for calculating zero_point\n",
        "    zero_point = 0\n",
        "    ############### YOUR CODE ENDS HERE #################\n",
        "\n",
        "    # clip the zero_point to fall in [quantized_min, quantized_max]\n",
        "    if zero_point < quantized_min:\n",
        "        zero_point = quantized_min\n",
        "    elif zero_point > quantized_max:\n",
        "        zero_point = quantized_max\n",
        "    else: # convert from float to int using round()\n",
        "        zero_point = round(zero_point)\n",
        "    return scale, int(zero_point)"
      ],
      "metadata": {
        "id": "LfAjS4KhfwDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now wrap  `linear_quantize()` in Question 4 and `get_quantization_scale_and_zero_point()` in Question 5 into one function."
      ],
      "metadata": {
        "id": "OW1AOZboSK0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_quantize_feature(fp_tensor, bitwidth):\n",
        "    \"\"\"\n",
        "    linear quantization for feature tensor\n",
        "    :param fp_tensor: [torch.(cuda.)Tensor] floating feature to be quantized\n",
        "    :param bitwidth: [int] quantization bit width\n",
        "    :return:\n",
        "        [torch.(cuda.)Tensor] quantized tensor\n",
        "        [float] scale tensor\n",
        "        [int] zero point\n",
        "    \"\"\"\n",
        "    scale, zero_point = get_quantization_scale_and_zero_point(fp_tensor, bitwidth)\n",
        "    quantized_tensor = linear_quantize(fp_tensor, bitwidth, scale, zero_point)\n",
        "    return quantized_tensor, scale, zero_point"
      ],
      "metadata": {
        "id": "FWeOwwXgoPLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Special case: linear quantization on weight tensor\n",
        "\n",
        "Let's first see the distribution of weight values."
      ],
      "metadata": {
        "id": "a0YWp1YtfY-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_weight_distribution(model, bitwidth=32):\n",
        "    # bins = (1 << bitwidth) if bitwidth <= 8 else 256\n",
        "    if bitwidth <= 8:\n",
        "        qmin, qmax = get_quantized_range(bitwidth)\n",
        "        bins = np.arange(qmin, qmax + 2)\n",
        "        align = 'left'\n",
        "    else:\n",
        "        bins = 256\n",
        "        align = 'mid'\n",
        "    fig, axes = plt.subplots(3,3, figsize=(10, 6))\n",
        "    axes = axes.ravel()\n",
        "    plot_index = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.dim() > 1:\n",
        "            ax = axes[plot_index]\n",
        "            ax.hist(param.detach().view(-1).cpu(), bins=bins, density=True,\n",
        "                    align=align, color = 'blue', alpha = 0.5,\n",
        "                    edgecolor='black' if bitwidth <= 4 else None)\n",
        "            if bitwidth <= 4:\n",
        "                quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
        "                ax.set_xticks(np.arange(start=quantized_min, stop=quantized_max+1))\n",
        "            ax.set_xlabel(name)\n",
        "            ax.set_ylabel('density')\n",
        "            plot_index += 1\n",
        "    fig.suptitle(f'Histogram of Weights (bitwidth={bitwidth} bits)')\n",
        "    fig.tight_layout()\n",
        "    fig.subplots_adjust(top=0.925)\n",
        "    plt.show()\n",
        "\n",
        "recover_model()\n",
        "plot_weight_distribution(model)"
      ],
      "metadata": {
        "id": "-HdIc5kKk89j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the histograms above, the distribution of weight values are nearly symmetric about 0 (except for the classifier in this case). Therefore, we usually make zero point $Z=0$ when quantizating the weights.\n",
        "\n",
        "From $r = S(q-Z)$, we have\n",
        "\n",
        "> $r_{\\mathrm{max}} = S \\cdot q_{\\mathrm{max}}$\n",
        "\n",
        "and then\n",
        "\n",
        "> $S = r_{\\mathrm{max}} / q_{\\mathrm{max}}$\n",
        "\n",
        "We directly use the maximum magnitude of weight values as $r_{\\mathrm{max}}$."
      ],
      "metadata": {
        "id": "WpAmSr7IlDn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_quantization_scale_for_weight(weight, bitwidth):\n",
        "    \"\"\"\n",
        "    get quantization scale for single tensor of weight\n",
        "    :param weight: [torch.(cuda.)Tensor] floating weight to be quantized\n",
        "    :param bitwidth: [integer] quantization bit width\n",
        "    :return:\n",
        "        [floating scalar] scale\n",
        "    \"\"\"\n",
        "    # we just assume values in weight are symmetric\n",
        "    # we also always make zero_point 0 for weight\n",
        "    fp_max = max(weight.abs().max().item(), 5e-7)\n",
        "    _, quantized_max = get_quantized_range(bitwidth)\n",
        "    return fp_max / quantized_max"
      ],
      "metadata": {
        "id": "v3d-Y-G9enEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Per-channel Linear Quantization\n",
        "\n",
        "Recall that for 2D convolution, the weight tensor is a 4-D tensor in the shape of (num_output_channels, num_input_channels, kernel_height, kernel_width).\n",
        "\n",
        "Intensive experiments show that using the different scaling factors $S$ and zero points $Z$ for different output channels will perform better. Therefore, we have to determine scaling factor $S$ and zero point $Z$ for the subtensor of each output channel independently."
      ],
      "metadata": {
        "id": "X71SQhfDnwS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_quantize_weight_per_channel(tensor, bitwidth):\n",
        "    \"\"\"\n",
        "    linear quantization for weight tensor\n",
        "        using different scales and zero_points for different output channels\n",
        "    :param tensor: [torch.(cuda.)Tensor] floating weight to be quantized\n",
        "    :param bitwidth: [int] quantization bit width\n",
        "    :return:\n",
        "        [torch.(cuda.)Tensor] quantized tensor\n",
        "        [torch.(cuda.)Tensor] scale tensor\n",
        "        [int] zero point (which is always 0)\n",
        "    \"\"\"\n",
        "    dim_output_channels = 0\n",
        "    num_output_channels = tensor.shape[dim_output_channels]\n",
        "    scale = torch.zeros(num_output_channels, device=tensor.device)\n",
        "    for oc in range(num_output_channels):\n",
        "        _subtensor = tensor.select(dim_output_channels, oc)\n",
        "        _scale = get_quantization_scale_for_weight(_subtensor, bitwidth)\n",
        "        scale[oc] = _scale\n",
        "    scale_shape = [1] * tensor.dim()\n",
        "    scale_shape[dim_output_channels] = -1\n",
        "    scale = scale.view(scale_shape)\n",
        "    quantized_tensor = linear_quantize(tensor, bitwidth, scale, zero_point=0)\n",
        "    return quantized_tensor, scale, 0"
      ],
      "metadata": {
        "id": "tFkA5JlgoiLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Quick Peek at Linear Quantization on Weights\n",
        "\n",
        "Now let's have a peek on the weight distribution and model size when applying linear quantization on weights with different bitwidths."
      ],
      "metadata": {
        "id": "bqKnjcUJpq2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def peek_linear_quantization():\n",
        "    for bitwidth in [4, 2]:\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.dim() > 1:\n",
        "                quantized_param, scale, zero_point = \\\n",
        "                    linear_quantize_weight_per_channel(param, bitwidth)\n",
        "                param.copy_(quantized_param)\n",
        "        plot_weight_distribution(model, bitwidth)\n",
        "        recover_model()\n",
        "\n",
        "peek_linear_quantization()"
      ],
      "metadata": {
        "id": "6PzZe2DGvWap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantized Inference"
      ],
      "metadata": {
        "id": "5Dw14QcxxItm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After quantization, the inference of convolution and fully-connected layers also change.\n",
        "\n",
        "Recall that $r = S(q-Z)$, and we have\n",
        "\n",
        "> $r_{\\mathrm{input}} = S_{\\mathrm{input}}(q_{\\mathrm{input}}-Z_{\\mathrm{input}})$\n",
        ">\n",
        "> $r_{\\mathrm{weight}} = S_{\\mathrm{weight}}(q_{\\mathrm{weight}}-Z_{\\mathrm{weight}})$\n",
        ">\n",
        "> $r_{\\mathrm{bias}} = S_{\\mathrm{bias}}(q_{\\mathrm{bias}}-Z_{\\mathrm{bias}})$\n",
        "\n",
        "Since $Z_{\\mathrm{weight}}=0$, $r_{\\mathrm{weight}} = S_{\\mathrm{weight}}q_{\\mathrm{weight}}$.\n",
        "\n",
        "The floating point convolution can be written as,\n",
        "\n",
        "> $r_{\\mathrm{output}} = \\mathrm{CONV}[r_{\\mathrm{input}}, r_{\\mathrm{weight}}] + r_{\\mathrm{bias}}\\\\\n",
        "\\;\\;\\;\\;\\;\\;\\;\\;= \\mathrm{CONV}[S_{\\mathrm{input}}(q_{\\mathrm{input}}-Z_{\\mathrm{input}}), S_{\\mathrm{weight}}q_{\\mathrm{weight}}] + S_{\\mathrm{bias}}(q_{\\mathrm{bias}}-Z_{\\mathrm{bias}})\\\\\n",
        "\\;\\;\\;\\;\\;\\;\\;\\;= \\mathrm{CONV}[q_{\\mathrm{input}}-Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]\\cdot (S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}}) + S_{\\mathrm{bias}}(q_{\\mathrm{bias}}-Z_{\\mathrm{bias}})$\n",
        "\n",
        "To further simplify the computation, we could let\n",
        "\n",
        "> $Z_{\\mathrm{bias}} = 0$\n",
        ">\n",
        "> $S_{\\mathrm{bias}} = S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}}$\n",
        "\n",
        "so that\n",
        "\n",
        "> $r_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}-Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}})$\n",
        "> $\\;\\;\\;\\;\\;\\;\\;\\;= (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}})$\n",
        "\n",
        "Since\n",
        "> $r_{\\mathrm{output}} = S_{\\mathrm{output}}(q_{\\mathrm{output}}-Z_{\\mathrm{output}})$\n",
        "\n",
        "we have\n",
        "> $S_{\\mathrm{output}}(q_{\\mathrm{output}}-Z_{\\mathrm{output}}) = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} S_{\\mathrm{weight}})$\n",
        "\n",
        "and thus\n",
        "> $q_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}$\n",
        "\n",
        "Since $Z_{\\mathrm{input}}$, $q_{\\mathrm{weight}}$, $q_{\\mathrm{bias}}$ are determined before inference, let\n",
        "\n",
        "> $Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]$\n",
        "\n",
        "we have\n",
        "\n",
        "> $q_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}}) \\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}$\n",
        "\n",
        "Similarily, for fully-connected layer, we have\n",
        "\n",
        "> $q_{\\mathrm{output}} = (\\mathrm{Linear}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}$\n",
        "\n",
        "where\n",
        "\n",
        "> $Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{Linear}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]$"
      ],
      "metadata": {
        "id": "zsHy-Bx-UfpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 6 (5 pts)\n",
        "\n",
        "Please complete the following function for linear quantizing the bias.\n",
        "\n",
        "**Hint**:\n",
        "\n",
        "From the above deduction, we know that\n",
        "\n",
        "> $Z_{\\mathrm{bias}} = 0$\n",
        ">\n",
        "> $S_{\\mathrm{bias}} = S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}}$"
      ],
      "metadata": {
        "id": "XlH0zg7M2J_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_quantize_bias_per_output_channel(bias, weight_scale, input_scale):\n",
        "    \"\"\"\n",
        "    linear quantization for single bias tensor\n",
        "        quantized_bias = fp_bias / bias_scale\n",
        "    :param bias: [torch.FloatTensor] bias weight to be quantized\n",
        "    :param weight_scale: [float or torch.FloatTensor] weight scale tensor\n",
        "    :param input_scale: [float] input scale\n",
        "    :return:\n",
        "        [torch.IntTensor] quantized bias tensor\n",
        "    \"\"\"\n",
        "    assert(bias.dim() == 1)\n",
        "    assert(bias.dtype == torch.float)\n",
        "    assert(isinstance(input_scale, float))\n",
        "    if isinstance(weight_scale, torch.Tensor):\n",
        "        assert(weight_scale.dtype == torch.float)\n",
        "        weight_scale = weight_scale.view(-1)\n",
        "        assert(bias.numel() == weight_scale.numel())\n",
        "\n",
        "    ############### YOUR CODE STARTS HERE ###############\n",
        "    # hint: one line of code\n",
        "    bias_scale = 0\n",
        "    ############### YOUR CODE ENDS HERE #################\n",
        "\n",
        "    quantized_bias = linear_quantize(bias, 32, bias_scale,\n",
        "                                     zero_point=0, dtype=torch.int32)\n",
        "    return quantized_bias, bias_scale, 0"
      ],
      "metadata": {
        "id": "0JZiyAms2G1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantized Fully-Connected Layer"
      ],
      "metadata": {
        "id": "mMM7uYX25rFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For quantized fully-connected layer, we first precompute $Q_{\\mathrm{bias}}$. Recall that $Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{Linear}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]$."
      ],
      "metadata": {
        "id": "znsT4EWL5tA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shift_quantized_linear_bias(quantized_bias, quantized_weight, input_zero_point):\n",
        "    \"\"\"\n",
        "    shift quantized bias to incorporate input_zero_point for nn.Linear\n",
        "        shifted_quantized_bias = quantized_bias - Linear(input_zero_point, quantized_weight)\n",
        "    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)\n",
        "    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)\n",
        "    :param input_zero_point: [int] input zero point\n",
        "    :return:\n",
        "        [torch.IntTensor] shifted quantized bias tensor\n",
        "    \"\"\"\n",
        "    assert(quantized_bias.dtype == torch.int32)\n",
        "    assert(isinstance(input_zero_point, int))\n",
        "    return quantized_bias - quantized_weight.sum(1).to(torch.int32) * input_zero_point"
      ],
      "metadata": {
        "id": "4rnNs4MN5tgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 7 (15 pts)\n",
        "\n",
        "Please complete the following quantized fully-connected layer inference function.\n",
        "\n",
        "**Hint**:\n",
        "\n",
        "> $q_{\\mathrm{output}} = (\\mathrm{Linear}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}$"
      ],
      "metadata": {
        "id": "DqNMxMzk3cDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantized_linear(input, weight, bias, feature_bitwidth, weight_bitwidth,\n",
        "                     input_zero_point, output_zero_point,\n",
        "                     input_scale, weight_scale, output_scale):\n",
        "    \"\"\"\n",
        "    quantized fully-connected layer\n",
        "    :param input: [torch.CharTensor] quantized input (torch.int8)\n",
        "    :param weight: [torch.CharTensor] quantized weight (torch.int8)\n",
        "    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)\n",
        "    :param feature_bitwidth: [int] quantization bit width of input and output\n",
        "    :param weight_bitwidth: [int] quantization bit width of weight\n",
        "    :param input_zero_point: [int] input zero point\n",
        "    :param output_zero_point: [int] output zero point\n",
        "    :param input_scale: [float] input feature scale\n",
        "    :param weight_scale: [torch.FloatTensor] weight per-channel scale\n",
        "    :param output_scale: [float] output feature scale\n",
        "    :return:\n",
        "        [torch.CharIntTensor] quantized output feature (torch.int8)\n",
        "    \"\"\"\n",
        "    assert(input.dtype == torch.int8)\n",
        "    assert(weight.dtype == input.dtype)\n",
        "    assert(bias is None or bias.dtype == torch.int32)\n",
        "    assert(isinstance(input_zero_point, int))\n",
        "    assert(isinstance(output_zero_point, int))\n",
        "    assert(isinstance(input_scale, float))\n",
        "    assert(isinstance(output_scale, float))\n",
        "    assert(weight_scale.dtype == torch.float)\n",
        "\n",
        "    # Step 1: integer-based fully-connected (8-bit multiplication with 32-bit accumulation)\n",
        "    if 'cpu' in input.device.type:\n",
        "        # use 32-b MAC for simplicity\n",
        "        output = torch.nn.functional.linear(input.to(torch.int32), weight.to(torch.int32), bias)\n",
        "    else:\n",
        "        # current version pytorch does not yet support integer-based linear() on GPUs\n",
        "        output = torch.nn.functional.linear(input.float(), weight.float(), bias.float())\n",
        "\n",
        "    ############### YOUR CODE STARTS HERE ###############\n",
        "    # Step 2: scale the output\n",
        "    #         hint: 1. scales are floating numbers, we need to convert output to float as well\n",
        "    #               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc]\n",
        "    output = 0\n",
        "\n",
        "    # Step 3: shift output by output_zero_point\n",
        "    #         hint: one line of code\n",
        "    output = 0\n",
        "    ############### YOUR CODE ENDS HERE #################\n",
        "\n",
        "    # Make sure all value lies in the bitwidth-bit range\n",
        "    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n",
        "    return output"
      ],
      "metadata": {
        "id": "3PVvI7jP3cMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's verify the functionality of defined quantized fully connected layer."
      ],
      "metadata": {
        "id": "115enVamIG_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_quantized_fc()"
      ],
      "metadata": {
        "id": "HWmsLxgHH_B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantized Convolution"
      ],
      "metadata": {
        "id": "ATooyRrH50ls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For quantized convolution layer, we first precompute $Q_{\\mathrm{bias}}$. Recall that $Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]$."
      ],
      "metadata": {
        "id": "8mk1Ziof51JG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shift_quantized_conv2d_bias(quantized_bias, quantized_weight, input_zero_point):\n",
        "    \"\"\"\n",
        "    shift quantized bias to incorporate input_zero_point for nn.Conv2d\n",
        "        shifted_quantized_bias = quantized_bias - Conv(input_zero_point, quantized_weight)\n",
        "    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)\n",
        "    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)\n",
        "    :param input_zero_point: [int] input zero point\n",
        "    :return:\n",
        "        [torch.IntTensor] shifted quantized bias tensor\n",
        "    \"\"\"\n",
        "    assert(quantized_bias.dtype == torch.int32)\n",
        "    assert(isinstance(input_zero_point, int))\n",
        "    return quantized_bias - quantized_weight.sum((1,2,3)).to(torch.int32) * input_zero_point"
      ],
      "metadata": {
        "id": "wEeANE_I53hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 8 (15 pts)\n",
        "\n",
        "Please complete the following quantized convolution function.\n",
        "\n",
        "**Hint**:\n",
        "> $q_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}}) \\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}$\n"
      ],
      "metadata": {
        "id": "0x2SqxOp23cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantized_conv2d(input, weight, bias, feature_bitwidth, weight_bitwidth,\n",
        "                     input_zero_point, output_zero_point,\n",
        "                     input_scale, weight_scale, output_scale,\n",
        "                     stride, padding, dilation, groups):\n",
        "    \"\"\"\n",
        "    quantized 2d convolution\n",
        "    :param input: [torch.CharTensor] quantized input (torch.int8)\n",
        "    :param weight: [torch.CharTensor] quantized weight (torch.int8)\n",
        "    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)\n",
        "    :param feature_bitwidth: [int] quantization bit width of input and output\n",
        "    :param weight_bitwidth: [int] quantization bit width of weight\n",
        "    :param input_zero_point: [int] input zero point\n",
        "    :param output_zero_point: [int] output zero point\n",
        "    :param input_scale: [float] input feature scale\n",
        "    :param weight_scale: [torch.FloatTensor] weight per-channel scale\n",
        "    :param output_scale: [float] output feature scale\n",
        "    :return:\n",
        "        [torch.(cuda.)CharTensor] quantized output feature\n",
        "    \"\"\"\n",
        "    assert(len(padding) == 4)\n",
        "    assert(input.dtype == torch.int8)\n",
        "    assert(weight.dtype == input.dtype)\n",
        "    assert(bias is None or bias.dtype == torch.int32)\n",
        "    assert(isinstance(input_zero_point, int))\n",
        "    assert(isinstance(output_zero_point, int))\n",
        "    assert(isinstance(input_scale, float))\n",
        "    assert(isinstance(output_scale, float))\n",
        "    assert(weight_scale.dtype == torch.float)\n",
        "\n",
        "    # Step 1: calculate integer-based 2d convolution (8-bit multiplication with 32-bit accumulation)\n",
        "    input = torch.nn.functional.pad(input, padding, 'constant', input_zero_point)\n",
        "    if 'cpu' in input.device.type:\n",
        "        # use 32-b MAC for simplicity\n",
        "        output = torch.nn.functional.conv2d(input.to(torch.int32), weight.to(torch.int32), None, stride, 0, dilation, groups)\n",
        "    else:\n",
        "        # current version pytorch does not yet support integer-based conv2d() on GPUs\n",
        "        output = torch.nn.functional.conv2d(input.float(), weight.float(), None, stride, 0, dilation, groups)\n",
        "        output = output.round().to(torch.int32)\n",
        "    if bias is not None:\n",
        "        output = output + bias.view(1, -1, 1, 1)\n",
        "\n",
        "    ############### YOUR CODE STARTS HERE ###############\n",
        "    # hint: this code block should be the very similar to quantized_linear()\n",
        "\n",
        "    # Step 2: scale the output\n",
        "    #         hint: 1. scales are floating numbers, we need to convert output to float as well\n",
        "    #               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc, height, width]\n",
        "    output = 0\n",
        "\n",
        "    # Step 3: shift output by output_zero_point\n",
        "    #         hint: one line of code\n",
        "    output = 0\n",
        "    ############### YOUR CODE ENDS HERE #################\n",
        "\n",
        "    # Make sure all value lies in the bitwidth-bit range\n",
        "    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n",
        "    return output"
      ],
      "metadata": {
        "id": "LVRqhiUno65x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 9 (10 pts)\n",
        "\n",
        "Finally, we are putting everything together and perform post-training `int8` quantization for the model. We will convert the convolutional and linear layers in the model to a quantized version one-by-one."
      ],
      "metadata": {
        "id": "32vvQ4WJHVlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Firstly, we will fuse a BatchNorm layer into its previous convolutional layer, which is a standard practice before quantization. Fusing batchnorm reduces the extra multiplication during inference.\n",
        "\n",
        "We will also verify that the fused model `model_fused` has the same accuracy as the original model (BN fusion is an equivalent transform that does not change network functionality)."
      ],
      "metadata": {
        "id": "2E5EDR2sINVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fuse_conv_bn(conv, bn):\n",
        "    # modified from https://mmcv.readthedocs.io/en/latest/_modules/mmcv/cnn/utils/fuse_conv_bn.html\n",
        "    assert conv.bias is None\n",
        "\n",
        "    factor = bn.weight.data / torch.sqrt(bn.running_var.data + bn.eps)\n",
        "    conv.weight.data = conv.weight.data * factor.reshape(-1, 1, 1, 1)\n",
        "    conv.bias = nn.Parameter(- bn.running_mean.data * factor + bn.bias.data)\n",
        "\n",
        "    return conv\n",
        "\n",
        "print('Before conv-bn fusion: backbone length', len(model.backbone))\n",
        "#  fuse the batchnorm into conv layers\n",
        "recover_model()\n",
        "model_fused = copy.deepcopy(model)\n",
        "fused_backbone = []\n",
        "ptr = 0\n",
        "while ptr < len(model_fused.backbone):\n",
        "    if isinstance(model_fused.backbone[ptr], nn.Conv2d) and \\\n",
        "        isinstance(model_fused.backbone[ptr + 1], nn.BatchNorm2d):\n",
        "        fused_backbone.append(fuse_conv_bn(\n",
        "            model_fused.backbone[ptr], model_fused.backbone[ptr+ 1]))\n",
        "        ptr += 2\n",
        "    else:\n",
        "        fused_backbone.append(model_fused.backbone[ptr])\n",
        "        ptr += 1\n",
        "model_fused.backbone = nn.Sequential(*fused_backbone)\n",
        "\n",
        "print('After conv-bn fusion: backbone length', len(model_fused.backbone))\n",
        "# sanity check, no BN anymore\n",
        "for m in model_fused.modules():\n",
        "    assert not isinstance(m, nn.BatchNorm2d)\n",
        "\n",
        "#  the accuracy will remain the same after fusion\n",
        "fused_acc = evaluate(model_fused, dataloader['test'])\n",
        "print(f'Accuracy of the fused model={fused_acc:.2f}%')"
      ],
      "metadata": {
        "id": "V2K8KSl7IE4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. We will run the model with some sample data to get the range of each feature map, so that we can get the range of the feature maps and compute their corresponding scaling factors and zero points."
      ],
      "metadata": {
        "id": "oCOXprerMY5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add hook to record the min max value of the activation\n",
        "input_activation = {}\n",
        "output_activation = {}\n",
        "\n",
        "def add_range_recoder_hook(model):\n",
        "    import functools\n",
        "    def _record_range(self, x, y, module_name):\n",
        "        x = x[0]\n",
        "        input_activation[module_name] = x.detach()\n",
        "        output_activation[module_name] = y.detach()\n",
        "\n",
        "    all_hooks = []\n",
        "    for name, m in model.named_modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear, nn.ReLU)):\n",
        "            all_hooks.append(m.register_forward_hook(\n",
        "                functools.partial(_record_range, module_name=name)))\n",
        "    return all_hooks\n",
        "\n",
        "hooks = add_range_recoder_hook(model_fused)\n",
        "sample_data = iter(dataloader['train']).__next__()[0]\n",
        "model_fused(sample_data.cuda())\n",
        "\n",
        "# remove hooks\n",
        "for h in hooks:\n",
        "    h.remove()\n"
      ],
      "metadata": {
        "id": "CjKrC7L-UrxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Finally, let's do model quantization. We will convert the model in the following mapping\n",
        "```python\n",
        "nn.Conv2d: QuantizedConv2d,\n",
        "nn.Linear: QuantizedLinear,\n",
        "# the following twos are just wrappers, as current\n",
        "# torch modules do not support int8 data format;\n",
        "# we will temporarily convert them to fp32 for computation\n",
        "nn.MaxPool2d: QuantizedMaxPool2d,\n",
        "nn.AvgPool2d: QuantizedAvgPool2d,\n",
        "```"
      ],
      "metadata": {
        "id": "uq3fvyx_6bLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedConv2d(nn.Module):\n",
        "    def __init__(self, weight, bias,\n",
        "                 input_zero_point, output_zero_point,\n",
        "                 input_scale, weight_scale, output_scale,\n",
        "                 stride, padding, dilation, groups,\n",
        "                 feature_bitwidth=8, weight_bitwidth=8):\n",
        "        super().__init__()\n",
        "        # current version Pytorch does not support IntTensor as nn.Parameter\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.register_buffer('bias', bias)\n",
        "\n",
        "        self.input_zero_point = input_zero_point\n",
        "        self.output_zero_point = output_zero_point\n",
        "\n",
        "        self.input_scale = input_scale\n",
        "        self.register_buffer('weight_scale', weight_scale)\n",
        "        self.output_scale = output_scale\n",
        "\n",
        "        self.stride = stride\n",
        "        self.padding = (padding[1], padding[1], padding[0], padding[0])\n",
        "        self.dilation = dilation\n",
        "        self.groups = groups\n",
        "\n",
        "        self.feature_bitwidth = feature_bitwidth\n",
        "        self.weight_bitwidth = weight_bitwidth\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return quantized_conv2d(\n",
        "            x, self.weight, self.bias,\n",
        "            self.feature_bitwidth, self.weight_bitwidth,\n",
        "            self.input_zero_point, self.output_zero_point,\n",
        "            self.input_scale, self.weight_scale, self.output_scale,\n",
        "            self.stride, self.padding, self.dilation, self.groups\n",
        "            )\n",
        "\n",
        "class QuantizedLinear(nn.Module):\n",
        "    def __init__(self, weight, bias,\n",
        "                 input_zero_point, output_zero_point,\n",
        "                 input_scale, weight_scale, output_scale,\n",
        "                 feature_bitwidth=8, weight_bitwidth=8):\n",
        "        super().__init__()\n",
        "        # current version Pytorch does not support IntTensor as nn.Parameter\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.register_buffer('bias', bias)\n",
        "\n",
        "        self.input_zero_point = input_zero_point\n",
        "        self.output_zero_point = output_zero_point\n",
        "\n",
        "        self.input_scale = input_scale\n",
        "        self.register_buffer('weight_scale', weight_scale)\n",
        "        self.output_scale = output_scale\n",
        "\n",
        "        self.feature_bitwidth = feature_bitwidth\n",
        "        self.weight_bitwidth = weight_bitwidth\n",
        "\n",
        "    def forward(self, x):\n",
        "        return quantized_linear(\n",
        "            x, self.weight, self.bias,\n",
        "            self.feature_bitwidth, self.weight_bitwidth,\n",
        "            self.input_zero_point, self.output_zero_point,\n",
        "            self.input_scale, self.weight_scale, self.output_scale\n",
        "            )\n",
        "\n",
        "class QuantizedMaxPool2d(nn.MaxPool2d):\n",
        "    def forward(self, x):\n",
        "        # current version PyTorch does not support integer-based MaxPool\n",
        "        return super().forward(x.float()).to(torch.int8)\n",
        "\n",
        "class QuantizedAvgPool2d(nn.AvgPool2d):\n",
        "    def forward(self, x):\n",
        "        # current version PyTorch does not support integer-based AvgPool\n",
        "        return super().forward(x.float()).to(torch.int8)\n",
        "\n",
        "# we use int8 quantization, which is quite popular\n",
        "feature_bitwidth = weight_bitwidth = 8\n",
        "quantized_model = copy.deepcopy(model_fused)\n",
        "quantized_backbone = []\n",
        "ptr = 0\n",
        "while ptr < len(quantized_model.backbone):\n",
        "    if isinstance(quantized_model.backbone[ptr], nn.Conv2d) and \\\n",
        "        isinstance(quantized_model.backbone[ptr + 1], nn.ReLU):\n",
        "        conv = quantized_model.backbone[ptr]\n",
        "        conv_name = f'backbone.{ptr}'\n",
        "        relu = quantized_model.backbone[ptr + 1]\n",
        "        relu_name = f'backbone.{ptr + 1}'\n",
        "\n",
        "        input_scale, input_zero_point = \\\n",
        "            get_quantization_scale_and_zero_point(\n",
        "                input_activation[conv_name], feature_bitwidth)\n",
        "\n",
        "        output_scale, output_zero_point = \\\n",
        "            get_quantization_scale_and_zero_point(\n",
        "                output_activation[relu_name], feature_bitwidth)\n",
        "\n",
        "        quantized_weight, weight_scale, weight_zero_point = \\\n",
        "            linear_quantize_weight_per_channel(conv.weight.data, weight_bitwidth)\n",
        "        quantized_bias, bias_scale, bias_zero_point = \\\n",
        "            linear_quantize_bias_per_output_channel(\n",
        "                conv.bias.data, weight_scale, input_scale)\n",
        "        shifted_quantized_bias = \\\n",
        "            shift_quantized_conv2d_bias(quantized_bias, quantized_weight,\n",
        "                                        input_zero_point)\n",
        "\n",
        "        quantized_conv = QuantizedConv2d(\n",
        "            quantized_weight, shifted_quantized_bias,\n",
        "            input_zero_point, output_zero_point,\n",
        "            input_scale, weight_scale, output_scale,\n",
        "            conv.stride, conv.padding, conv.dilation, conv.groups,\n",
        "            feature_bitwidth=feature_bitwidth, weight_bitwidth=weight_bitwidth\n",
        "        )\n",
        "\n",
        "        quantized_backbone.append(quantized_conv)\n",
        "        ptr += 2\n",
        "    elif isinstance(quantized_model.backbone[ptr], nn.MaxPool2d):\n",
        "        quantized_backbone.append(QuantizedMaxPool2d(\n",
        "            kernel_size=quantized_model.backbone[ptr].kernel_size,\n",
        "            stride=quantized_model.backbone[ptr].stride\n",
        "            ))\n",
        "        ptr += 1\n",
        "    elif isinstance(quantized_model.backbone[ptr], nn.AvgPool2d):\n",
        "        quantized_backbone.append(QuantizedAvgPool2d(\n",
        "            kernel_size=quantized_model.backbone[ptr].kernel_size,\n",
        "            stride=quantized_model.backbone[ptr].stride\n",
        "            ))\n",
        "        ptr += 1\n",
        "    else:\n",
        "        raise NotImplementedError(type(quantized_model.backbone[ptr]))  # should not happen\n",
        "quantized_model.backbone = nn.Sequential(*quantized_backbone)\n",
        "\n",
        "# finally, quantized the classifier\n",
        "fc_name = 'classifier'\n",
        "fc = model.classifier\n",
        "input_scale, input_zero_point = \\\n",
        "    get_quantization_scale_and_zero_point(\n",
        "        input_activation[fc_name], feature_bitwidth)\n",
        "\n",
        "output_scale, output_zero_point = \\\n",
        "    get_quantization_scale_and_zero_point(\n",
        "        output_activation[fc_name], feature_bitwidth)\n",
        "\n",
        "quantized_weight, weight_scale, weight_zero_point = \\\n",
        "    linear_quantize_weight_per_channel(fc.weight.data, weight_bitwidth)\n",
        "quantized_bias, bias_scale, bias_zero_point = \\\n",
        "    linear_quantize_bias_per_output_channel(\n",
        "        fc.bias.data, weight_scale, input_scale)\n",
        "shifted_quantized_bias = \\\n",
        "    shift_quantized_linear_bias(quantized_bias, quantized_weight,\n",
        "                                input_zero_point)\n",
        "\n",
        "quantized_model.classifier = QuantizedLinear(\n",
        "    quantized_weight, shifted_quantized_bias,\n",
        "    input_zero_point, output_zero_point,\n",
        "    input_scale, weight_scale, output_scale,\n",
        "    feature_bitwidth=feature_bitwidth, weight_bitwidth=weight_bitwidth\n",
        ")"
      ],
      "metadata": {
        "id": "iFD5W0H1Zban"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The quantization process is done! Let's print and visualize the model architecture and also verify the accuracy of the quantized model.\n"
      ],
      "metadata": {
        "id": "YIPgF84IygCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 9.1 (5 pts)\n",
        "\n",
        "To run the quantized model, we need an extra preprocessing to map the input data from range (0, 1) into `int8` range of (-128, 127). Fill in the code below to finish the extra preprocessing.\n",
        "\n",
        "**Hint**: you should find that the quantized model has roughly the same accuracy as the `fp32` counterpart."
      ],
      "metadata": {
        "id": "vGkf1sf27uFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(quantized_model)\n",
        "\n",
        "def extra_preprocess(x):\n",
        "    # hint: you need to convert the original fp32 input of range (0, 1)\n",
        "    #  into int8 format of range (-128, 127)\n",
        "    ############### YOUR CODE STARTS HERE ###############\n",
        "    return 0.clamp(-128, 127).to(torch.int8)\n",
        "    ############### YOUR CODE ENDS HERE #################\n",
        "\n",
        "int8_model_accuracy = evaluate(quantized_model, dataloader['test'],\n",
        "                               extra_preprocess=[extra_preprocess])\n",
        "print(f\"int8 model has accuracy={int8_model_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "PEZT47BE6dXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 9.2 (Bonus Question; 5 pts)\n",
        "\n",
        "Explain why there is no ReLU layer in the linear quantized model."
      ],
      "metadata": {
        "id": "61iwKRAA8A7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Answer:**"
      ],
      "metadata": {
        "id": "8Ewbt01T8F1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10 (5 pts)\n",
        "\n",
        "Please compare the advantages and disadvantages of k-means-based quantization and linear quantization. You can discuss from the perspective of accuracy, latency, hardware support, etc."
      ],
      "metadata": {
        "id": "HhSnPZpD7ye9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Answer:**"
      ],
      "metadata": {
        "id": "mhlf55UM73yA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feedback"
      ],
      "metadata": {
        "id": "xxOBqoXoSUfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please fill out this [feedback form](https://forms.gle/ZeCH5anNPrkd5wpp7) when you finished this lab. We would love to hear your thoughts or feedback on how we can improve this lab!"
      ],
      "metadata": {
        "id": "ajqZJes3SVc-"
      }
    }
  ]
}